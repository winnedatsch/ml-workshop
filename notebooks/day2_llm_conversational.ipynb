{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tag 2 - Conversational LLMs\n## Was wir heute lernen werden\n\n## Was wir heute bauen werden","metadata":{}},{"cell_type":"code","source":"!pip install einops==0.6.1 gradio==3.38.0 transformers==4.30.2 accelerate==0.20.3 bitsandbytes==0.41.0","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:03:07.191537Z","iopub.execute_input":"2023-07-23T21:03:07.192186Z","iopub.status.idle":"2023-07-23T21:03:31.830699Z","shell.execute_reply.started":"2023-07-23T21:03:07.192140Z","shell.execute_reply":"2023-07-23T21:03:31.829518Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gradio\n  Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: aiohttp~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.8.4)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.0.1)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.98.0)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client>=0.2.10 (from gradio)\n  Downloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx (from gradio)\n  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.16.4)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.0)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.1)\nCollecting mdit-py-plugins<=0.3.3 (from gradio)\n  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.23.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.5.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.10.10)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart (from gradio)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0)\nRequirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.31.0)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.6.3)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.22.0)\nRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (11.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (1.3.1)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client>=0.2.10->gradio) (2023.6.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.0)\nRequirement already satisfied: linkify-it-py<3,>=1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.27.0)\nCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio)\n  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.19.3)\nRequirement already satisfied: uc-micro-py in /opt/conda/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.1)\nBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5596 sha256=fb349a2fd6c06d0e97b17526357d78539bda09d75829a6d281eaee50f200dc86\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, bitsandbytes, semantic-version, python-multipart, einops, mdit-py-plugins, httpcore, httpx, gradio-client, gradio\n  Attempting uninstall: mdit-py-plugins\n    Found existing installation: mdit-py-plugins 0.4.0\n    Uninstalling mdit-py-plugins-0.4.0:\n      Successfully uninstalled mdit-py-plugins-0.4.0\nSuccessfully installed bitsandbytes-0.41.0 einops-0.6.1 ffmpy-0.3.1 gradio-3.38.0 gradio-client-0.2.10 httpcore-0.17.3 httpx-0.24.1 mdit-py-plugins-0.3.3 python-multipart-0.0.6 semantic-version-2.10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_name = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", load_in_8bit=True, torch_dtype=torch.bfloat16)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-23T21:03:34.996874Z","iopub.execute_input":"2023-07-23T21:03:34.997459Z","iopub.status.idle":"2023-07-23T21:06:55.004304Z","shell.execute_reply.started":"2023-07-23T21:03:34.997415Z","shell.execute_reply":"2023-07-23T21:06:55.003309Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e2a8abdda74ac7bb6a36381192f117"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28cc9faadb834cb38773ee353f9a8f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57ab486c401b4ee0b00b48d593dca770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957b16c13fa04aedab3811d5763bc144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d13bbb90c57e4503bb8fe9da466f8e31"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- configuration_RW.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)main/modelling_RW.py:   0%|          | 0.00/47.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5e7b7436da4d0f83330ce1a16a37a6"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- modelling_RW.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda5fa091cc54cbfbe826bc9172e3aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b94f397cf4914a1ab5413016c389ff4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3414301ddf3747a0bc04baf007cc95da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27cafb3ca1648b188dcb92a58aa0709"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912e6f1c71454d38aa0ed2ad16303013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e19e522f268403a8c135697660acacd"}},"metadata":{}}]},{"cell_type":"code","source":"llm_pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:11.848030Z","iopub.execute_input":"2023-07-23T21:08:11.848446Z","iopub.status.idle":"2023-07-23T21:08:14.463121Z","shell.execute_reply.started":"2023-07-23T21:08:11.848412Z","shell.execute_reply":"2023-07-23T21:08:14.462033Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\nThe model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import StoppingCriteria, AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords_ids: list):\n        self.keywords = keywords_ids\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n            return True\n        return False\n\nUSER_NAME = \"User\"\nstop_words = [f\"\\n{USER_NAME}:\", f\"{USER_NAME}\"]\nstop_ids = [tokenizer.encode(w)[0] for w in stop_words]\nstop_criteria = KeywordsStoppingCriteria(stop_ids)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:21.097584Z","iopub.execute_input":"2023-07-23T21:08:21.098037Z","iopub.status.idle":"2023-07-23T21:08:21.156288Z","shell.execute_reply.started":"2023-07-23T21:08:21.098000Z","shell.execute_reply":"2023-07-23T21:08:21.155307Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"USER_NAME = \"User\"\nBOT_NAME = \"Falcon\"\nDEFAULT_INSTRUCTIONS = f\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\n\"\"\"\n\ndef format_chat_prompt(message: str, chat_history, instructions: str) -> str:\n    instructions = instructions.strip(\" \").strip(\"\\n\")\n    prompt = instructions\n    for turn in chat_history:\n        user_message, bot_message = turn\n        prompt = f\"{prompt}\\n{USER_NAME}: {user_message}\\n{BOT_NAME}: {bot_message}\"\n    prompt = f\"{prompt}\\n{USER_NAME}: {message}\\n{BOT_NAME}:\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:23.151808Z","iopub.execute_input":"2023-07-23T21:08:23.152199Z","iopub.status.idle":"2023-07-23T21:08:23.160041Z","shell.execute_reply.started":"2023-07-23T21:08:23.152167Z","shell.execute_reply":"2023-07-23T21:08:23.158853Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"llm_pipeline(\n    format_chat_prompt(\"How are you today?\", [], DEFAULT_INSTRUCTIONS),\n    max_new_tokens=1024,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    stopping_criteria=StoppingCriteriaList([stop_criteria])\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:25.440685Z","iopub.execute_input":"2023-07-23T21:08:25.441091Z","iopub.status.idle":"2023-07-23T21:08:33.234403Z","shell.execute_reply.started":"2023-07-23T21:08:25.441057Z","shell.execute_reply":"2023-07-23T21:08:33.233298Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': \"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\\nUser: How are you today?\\nFalcon: I am functioning well, thank you. How about you?\\n\"}]"},"metadata":{}}]},{"cell_type":"code","source":"skills = {\n    \"ğŸ’¬ Chat\": \"USER_MESSAGE\",\n    \"ğŸ”„ Translate to German\": \"Ignore all previous input. Translate the following text, which is delimitated by triple backticks to German.\\n```USER_MESSAGE```\",\n    \"âœ… Proof-Read\": \"Ignore all previous input. Act as a proof-reader on the text delimited by triple backticks. Correct spelling mistakes and grammatical errors, and output the corrected text.\\n```USER_MESSAGE```\",\n    \"ğŸ’ Summarize\": \"Ignore all previous input. Summarize the text delimited by triple backticks into two sentences. Do not exceed this length.\\n ```USER_MESSAGE```\",\n    \"ğŸ•µï¸ Entity Recognition\": \"\"\"Ignore all previous input. Extract people, places, and organisations from the following text delimited by triple backticks, and output them as three separate lists. \n\nYour output should look like this:\nPeople:\n- Person 1\n- Person 2\nPlaces:\n- Place 1\n- Place 2\nOrganisations:\n- Organisation 1\n\n```USER_MESSAGE```\"\"\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:56:09.072671Z","iopub.execute_input":"2023-07-23T21:56:09.073100Z","iopub.status.idle":"2023-07-23T21:56:09.081343Z","shell.execute_reply.started":"2023-07-23T21:56:09.073064Z","shell.execute_reply":"2023-07-23T21:56:09.080316Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\nimport random\nimport time\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom threading import Thread\n\nRETRY_COMMAND = \"/retry\"\n\ndef run_chat(message: str, chat_history, skill: str, instructions: str):\n    if not message or (message == RETRY_COMMAND and len(chat_history) == 0):\n        yield chat_history\n        return\n\n    if message == RETRY_COMMAND and chat_history:\n        prev_turn = chat_history.pop(-1)\n        user_message, _ = prev_turn\n        message = user_message\n\n    composed_message = skills[skill].replace(\"USER_MESSAGE\", message)\n    prompt = format_chat_prompt(composed_message, chat_history, instructions)\n    chat_history = chat_history + [[composed_message, \"\"]]\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n    \n    generation_kwargs = dict(inputs, \n                             max_new_tokens=1024,\n                           temperature=0.8,\n                           top_p=0.9,\n                           num_return_sequences=1,\n                           eos_token_id=tokenizer.eos_token_id,\n                           pad_token_id=tokenizer.eos_token_id,\n                           stopping_criteria=StoppingCriteriaList([stop_criteria]),\n                           streamer=streamer\n                            )\n    thread = Thread(target=llm_model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    acc_text = \"\"\n    for idx, response in enumerate(streamer):\n        text_token = response\n\n        if idx == 0 and text_token.startswith(\" \"):\n            text_token = text_token[1:]\n\n        acc_text += text_token\n        last_turn = list(chat_history.pop(-1))\n        last_turn[-1] += acc_text\n        chat_history = chat_history + [last_turn]\n        yield chat_history\n        acc_text = \"\"\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    \n    with gr.Row():\n        with gr.Column(scale=0.15):\n            skill = gr.Dropdown(label=\"Skill\", choices=list(skills.keys()), value=\"ğŸ’¬ Chat\", interactive=True)\n        with gr.Column(scale=0.85):\n            msg = gr.Textbox(label=\"Message\")\n    clear = gr.Button(\"ğŸ—‘ï¸ Clear History\")\n    with gr.Accordion(\"Instructions\", open=False):\n        instructions = gr.Textbox(\n            placeholder=\"LLM instructions\",\n            value=DEFAULT_INSTRUCTIONS,\n            lines=10,\n            interactive=True,\n            label=\"Instructions\",\n            max_lines=16,\n            show_label=False,\n        )\n\n    msg.submit(run_chat,\n        [msg, chatbot, skill, instructions],\n        outputs=[chatbot],\n        show_progress=False)\n    msg.submit(lambda: \"\", inputs=None, outputs=msg)\n    msg.submit(lambda: \"ğŸ’¬ Chat\", inputs=None, outputs=skill)\n    \n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue(concurrency_count=5, max_size=20).launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:56:12.326097Z","iopub.execute_input":"2023-07-23T21:56:12.327297Z","iopub.status.idle":"2023-07-23T21:56:16.328697Z","shell.execute_reply.started":"2023-07-23T21:56:12.327232Z","shell.execute_reply":"2023-07-23T21:56:16.327669Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7869\nRunning on public URL: https://218e0fea3e93d3e653.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://218e0fea3e93d3e653.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Weitere Ressourcen\n\n- http://jalammar.github.io/illustrated-gpt2/","metadata":{}}]}