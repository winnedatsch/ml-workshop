{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag 2 - LLM Fine-Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung in das Hugging Face Ökosystem\n",
    "\n",
    "Um die Bestandteile des ML-Workflows und die technischen Details hinter ML-Modellen detaillierter zu beleuchten, haben wir gestern pures PyTorch verwendet, um unsere Daten zu laden und aufzubereiten, und unsere Modelle von Grund auf zu bauen, trainieren, und evaluieren. Für die meisten Aufgabenstellungen ist es heute jedoch üblich, bereits vortrainierte Modelle entweder direkt ohne Anpassungen zu verwenden (auch als *zero-shot* bezeichnet), oder diese für die eigene Problemstellung zu finetunen. \n",
    "\n",
    "Zu diesem Zweck hat sich [Hugging Face](https://huggingface.co/) sowohl in der Wissenschaft als auch im kommerziellen Bereich als die primäre Platform herauskristallisiert, um ML-Modelle, Datensätze, etc., auszutauschen. Zudem stellt Hugging Face eine weite Auswahl an Libraries zur Verfügung, die das Verwenden von bestehenden Modellen und Datensätzen, aber auch Aspekte wie Fine-Tuning, Optimierung für die Nutzung in Production, und Training auf Clustern oder spezieller Hardware (z.B. Google TPUs, Habana Gaudi Prozessoren) vereinfacht. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers\n",
    "Die wichtigste Hugging Face Library ist mit Abstand [Transformers](https://huggingface.co/docs/transformers/index). Sie kapselt die Nutzung von vortrainierten Modellen aus verschiedensten Einsatzgebieten, das für sie notwendige Pre- und Post-Processing, sowie ihr Training / Fine-Tuning.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "Zur Verwendung eines Modells aus dem Hugging Face Ökosystem benötigt man immer mindestens 2 Komponenten: die richtige Model-Klasse, und die Pre-Processors, die die Inputs für die aktuelle Aufgabenstellung (Bilder, Text, etc.) in das Format umwandeln, dass das Modell erwartet. Laden wir ein Beispiel-Modell, das Language Model RoBERTa (https://huggingface.co/roberta-base):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
    "\n",
    "roberta_mlm = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die vorangehenden Zeilen laden das RoBERTa Modell, und seinen Pre-Processor, den Tokenizer: Da ein Language Model (grob gesagt) numerischen Input erwartet, muss ein Eingabe-String zuerst in kleinere Teile aufgeteilt werden, die *Tokens*. Je nach Modell kommen hier unterschiedliche Techniken zum Einsatz, bei denen ein Token immer ein gesamtes Wort, oder häufiger den Teil eines Wortes repräsentiert (Sub-Word Tokenization). Die Verwendung der `from_pretrained` Methode mit dem selben Model-Identifier stellt sicher, dass wir die richtige Kombination and Modell und Tokenizer laden.\n",
    "\n",
    "<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Der Tokenizer von RoBERTa ist ein Byte Pair Encoding (BPE) Tokenizer. Diese Technik wurde erstmals von OpenAI GPT-2 verwendet, und wird hier gut erläutert: https://lucytalksdata.com/the-modern-tokenization-stack-for-nlp-byte-pair-encoding/. \n",
    "\n",
    "<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Um von der Transformer-Architektur, auf der fast alle modernen Language Models basieren, verwendet zu werden, müssen die Tokens (einfache Ids) noch weiter in multi-dimensionale Vektoren umgewandelt werden. Dieser Prozess wird als *Embedding* bezeichnet. Der Prozess von Tokenization zum Embedding wird in diesem Artikel sehr umfangreich erklärt: https://www.lesswrong.com/posts/pHPmMGEMYefk9jLeh/llm-basics-embedding-spaces-transformer-token-vectors-are. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie verwenden wir nun unser Modell? Wie du aus dem Code oben erkennen kannst, haben wir eine Variante von RoBERTa geladen, die für Masked Language Modelling (MLM) finetuned wurde, also das Füllen von Lücken in einem Text. Sehen wir uns ein Beispiel an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Paris'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "inputs = roberta_tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = roberta_mlm(**inputs).logits\n",
    "\n",
    "mask_token_index = (inputs.input_ids == roberta_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "roberta_tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie erwartet leiten wir unseren Input-Satz durch den Tokenizer, und geben das Ergebnis des Tokenizers 1-zu-1 and unser Modell weiter. Das Modell gibt für den maskierten Token (in diesem Fall enthält der Satz nur einen) Gewichte zurück (sogenannte Logits), die ausdrücken, welchen Token das Modell an dieser Stelle am wahrscheinlichsten einsetzen würde. Wie erwartet füllt das Modell die Lücke im Text mit der korrekten Antwort: Paris. \n",
    "\n",
    "<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Sieh dir die `inputs` an das Modell genauer an. Stimmt die Anzahl an Tokens (`input_ids`) mit der Anzahl an Wörtern überein, und wenn nicht, was könnte die Ursache sein (um eine Vermutung zu entkräftigen, sieh dir den Output von `inputs.word_ids(batch_index=0)` an)? Was könnte der Zweck der `attention_mask` sein? \n",
    "\n",
    "<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Leite dir ab, wie der Code oben genau funktioniert, um an die Antwort des Modells zu kommen. Tipps: Das Modell verarbeitet gewöhnlich nicht nur einen, sondern einen Batch an Sätzen. `tokenizer.mask_token_id` enthält die ID des Tokens, in den `<mask>` umgewandelt wird (also die ID, an der wir interessiert sind). Bei guten Verständnis solltest du den Code so umbauen können, dass du die Antworten für einen Batch an Sätzen auf einmal auslesen kannst. \n",
    "\n",
    "<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Transformers stellt sogenannte *Pipelines* zur Verfügung, die das Post-Processing der Model-Outputs übernehmen. Nutze die Pipelines API-Doc (https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/pipelines) um das gleiche Ergebnis zu erhalten. Übergib der Pipeline dabei die bereits initialisierten Modell & Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Details zum Laden von Modellen\n",
    "\n",
    "Wie bereits aus dem `with torch.no_grad()` Befehl erkennbar sein sollte, ist das mittels Transformers geladene Modell ein reguläres PyTorch Modell. Wir können uns daher die Details des Modells wie gewohnt ansehen, und auch sonst ohne die Helfer von Transformers mit dem Modell so interagieren, wie wir es von einem PyTorch Modell gewohnt sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RobertaForMaskedLM                                                --\n",
       "├─ModuleDict: 1-1                                                 --\n",
       "├─RobertaModel: 1-2                                               --\n",
       "│    └─ModuleDict: 2-1                                            --\n",
       "│    └─ModuleDict: 2-2                                            --\n",
       "│    └─RobertaEmbeddings: 2-3                                     --\n",
       "│    │    └─Embedding: 3-1                                        38,603,520\n",
       "│    │    └─Embedding: 3-2                                        394,752\n",
       "│    │    └─Embedding: 3-3                                        768\n",
       "│    │    └─LayerNorm: 3-4                                        1,536\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─RobertaEncoder: 2-4                                        --\n",
       "│    │    └─ModuleList: 3-6                                       85,054,464\n",
       "│    └─PrefixTuningPool: 2-5                                      --\n",
       "│    │    └─ModuleDict: 3-7                                       --\n",
       "├─RobertaLMHead: 1-3                                              --\n",
       "│    └─Linear: 2-6                                                590,592\n",
       "│    └─LayerNorm: 2-7                                             1,536\n",
       "│    └─Linear: 2-8                                                38,653,785\n",
       "==========================================================================================\n",
       "Total params: 163,300,953\n",
       "Trainable params: 163,300,953\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(roberta_mlm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Model-Summary deutet noch auf ein weiteres Feature von Transformers hin: Viele Modelle wurden nicht nur für einen, sondern mehrere Aufgaben vorbereitet. In unserem Fall sehen wir, dass unser `RobertaForMaskedLM` Modell 2 Bestandteile hat: Das Kern-RoBERTa Modell (`RobertaModel`) und eine Gruppe an zusätzlichen Layern, die den rohen Output des `RobertaEncoder`s in die Logit-Gewichte umwandelt, die wir oben gesehen haben (`RobertaLMHead`). Im Falle von RoBERTa wurden auch *Heads* für andere Aufgabenstellungen spezifiziert, unter anderem für jenen Task, mit dem wir uns als nächstes beschäftigen werden: Named Entity Recognition (NER). \n",
    "\n",
    "Um das Modell mit einem Head für NER zu laden, könnten wir die `RobertaForTokenClassification`-Klasse verwenden. Da Transformers die Interfaces der Modelle für die gleiche Aufgabenstellung standardisiert, können wir stattdessen eine der sogenannten `Auto`-Klassen verwenden, die die Details zum Modell automatisch aus seiner Spezifikation lädt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "roberta_ner = AutoModelForTokenClassification.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie wir an der Warnung erkennen, wurde zwar das Format für einen NER Head für RoBERTa spezifiziert, der Checkpoint `roberta-base` enthält jedoch keine Weights für diesen Head, weshalb wir ihn erst für unseren spezifischen Einsatzzweck trainieren müssen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Nach unserem etwas akademischen Beispiel widmen wir uns jetzt einem praktischeren Language-Model Task: Named Entity Recognition, oft auch Token Classification genannt. Hier sollen in einem unstrukturierten Text bestimmte Entitäten (Firmen, Personen, Orte, etc.) erkannt und extrahiert werden. Beispiele für Nutzungs-Szenarien sind:\n",
    "\n",
    "- Automatische Zuordnung von Support-Anfragen basierend auf den erkannten Regionen, Produkten, Herstellern, etc.\n",
    "- Automatisches Anreichern mit Kontext-Informationen, z.B. die aktuellen Aktienkurse für erkannte Organisationen in Zeitungs-Artikeln.\n",
    "- Intelligente Suchfunktionen, die beispielsweise eine medizinische Diagnose-Datenbank nach einer bestimmten Krankheits-Kategorie filtern.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Datasets\n",
    "\n",
    "Als Datensatz für unsere folgenden Experimente verwenden wir `bionlp2004` (https://huggingface.co/datasets/tner/bionlp2004), eine Sammlung aus medizinischen Fachtexten, in denen Fachbegriffe für Proteine, DNA, RNA, und Zellen hervorgehoben sind. Dieser Datensatz könnte somit als Beispiel für den Nutzen von NER zur Verwaltung von großen Wissens-Datenbanken gesehen werden, die fachspezifischen Freitext enthalten (man denke an die Confluence-Instanz eines großen Unternehmens). Ein anderer konkreter Use-Case eines Unternehmens wird mit hoher Wahrscheinlichkeit auch einen darauf angepassten Datensatz benötigen, für die Erklärung der technischen Prinzipien ist `bionlp2004` jedoch ausreichend. \n",
    "\n",
    "Ähnlich wie Hugging Face Transformers für Modelle gibt Hugging Face Datasets dem Entwickler die Möglichkeit, bekannte Referenz-Datensätze über ein simpel gestaltetes Interface zu laden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bionlp2004 (/Users/jan/.cache/huggingface/datasets/tner___bionlp2004/bionlp2004/1.0.0/9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffbb23d9de3486781dd4bb7b7b27fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "bionlp2004 = load_dataset(\"tner/bionlp2004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 16619\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 1927\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 3856\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bionlp2004"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen wir uns den geladenen Datensatz etwas genauer an. Der Datensatz wird bereits beim Laden in Train, Validation, und Test Splits aufgeteilt. Jeder dieser Splits ist eine Instanz der `Dataset` Klasse, die effizientes Pre-Processing der enthaltenen Daten ermöglicht und ausgezeichnet mit den restlichen Hugging Face Libraries (wie z.B. Transformers) zusammen arbeitet. \n",
    "\n",
    "<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Falls man die Features der Datasets Library auch mit eigenen Daten verwenden möchte, gibt es eine Reihe an Möglichkeiten, um ein Dataset zu konstruieren: z.B. aus [diversen Datei-Formaten](https://huggingface.co/docs/datasets/v2.13.1/en/package_reference/loading_methods#from-files), aus [Pandas `DataFrame`s](https://huggingface.co/docs/datasets/v2.13.1/en/package_reference/main_classes#datasets.Dataset.from_pandas), und aus [PyTorch `Dataset`s](https://github.com/huggingface/datasets/issues/4983).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Inhibition', 'by', 'anti-VCAM-1', 'monoclonal', 'antibody', '1G11', 'indicated', 'that', 'U937', 'adhesion', 'was', 'VCAM-1', 'dependent', 'and', 'suppression', 'by', 'antioxidants', 'was', 'due', 'to', 'reduced', 'VCAM-1', 'induction', '.'], 'tags': [0, 0, 3, 4, 4, 3, 0, 0, 7, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(bionlp2004[\"train\"][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die einzelnen Einträge des Datensatzes enthalten eine Reihe an `\"tokens\"` (wobei hier `\"tokens\"` einfach die einzelnen Worte des jeweiligen Texts beschreibt, nicht zu verwechseln mit den numerischen Tokens, die vom Tokenizer unseres Modells produziert werden), und dazugehörige `\"tags\"`, die jedem `token` eine Entitäts-Klasse zuordnen (oder 0, falls keine zutrifft). Um die numerischen Tag-Werte ihren Labeln zuordnen zu können, bauen wir uns Maps in beide Richtungen.\n",
    "\n",
    "<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Falls im eigenen Datensatz die Input-Texte noch nicht in Wörter unterteilt sind, kann dies beispielsweise mit dem `TreebankWordTokenzier` aus der NLTK Library erreicht werden: https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html. \n",
    "\n",
    "<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Unser Modell, RoBERTa, verwendet wie bereits erwähnt Sub-Word Tokenization. Welche Probleme siehst du darin, ein solches Modell für einen Einsatzzweck zu nutzen, in dem immer ganze Wörter oder Wortgruppen kategorisiert werden müssen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-DNA\": 1,\n",
    "    \"I-DNA\": 2,\n",
    "    \"B-protein\": 3,\n",
    "    \"I-protein\": 4,\n",
    "    \"B-cell_type\": 5,\n",
    "    \"I-cell_type\": 6,\n",
    "    \"B-cell_line\": 7,\n",
    "    \"I-cell_line\": 8,\n",
    "    \"B-RNA\": 9,\n",
    "    \"I-RNA\": 10,\n",
    "}\n",
    "\n",
    "id2label = {i: label for label, i in label2id.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit wir uns die Aufgabenstellung besser vorstellen können, nutzen wir die Library `spacy` um die Entitäten in einem Beispielsatz hervorzuheben.\n",
    "\n",
    "<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Die Token-Klassen folgen dem [IOB2-Schema](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), in dem bei zusammengehörigen Wörtern das erste mit \"B-\" Prefix, und alle zugehörigen Folgewörter mit \"I-\" Prefix klassifiziert werden. Ändere den folgenden Code so ab, dass zusammengehörige Wörter als einzelne Entität angezeigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Inhibition by \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    anti-VCAM-1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-protein</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    monoclonal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-protein</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    antibody\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-protein</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1G11\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-protein</span>\n",
       "</mark>\n",
       " indicated that \n",
       "<mark class=\"entity\" style=\"background: #CCCCCC; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U937\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-cell_line</span>\n",
       "</mark>\n",
       " adhesion was \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    VCAM-1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-protein</span>\n",
       "</mark>\n",
       " dependent and suppression by antioxidants was due to reduced \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    VCAM-1\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-protein</span>\n",
       "</mark>\n",
       " induction.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from spacy import displacy\n",
    "\n",
    "ner_colors = {\n",
    "    \"B-DNA\": \"#FF0000\",\n",
    "    \"I-DNA\": \"#FF0000\",\n",
    "    \"B-protein\": \"#FFA500\",\n",
    "    \"I-protein\": \"#FFA500\",\n",
    "    \"B-cell_type\": \"#088F8F\",\n",
    "    \"I-cell_type\": \"#088F8F\",\n",
    "    \"B-cell_line\": \"#CCCCCC\",\n",
    "    \"I-cell_line\": \"#CCCCCC\",\n",
    "    \"B-RNA\": \"#30D5C8\",\n",
    "    \"I-RNA\": \"#30D5C8\"\n",
    "}\n",
    "\n",
    "def show_ground_truth_with_spacy(sample):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    text = detokenizer.detokenize(sample[\"tokens\"])\n",
    "    entities = []\n",
    "    current_start = 0\n",
    "    \n",
    "    for word, tag in zip(sample[\"tokens\"], sample[\"tags\"]):\n",
    "        if tag != 0:\n",
    "            word_start = text.find(word, current_start)\n",
    "            word_end = word_start + len(word)\n",
    "            entities.append({\"label\": id2label[tag], \"start\": word_start, \"end\": word_end})\n",
    "            current_start = word_end\n",
    "\n",
    "    displacy.render({\"ents\": entities, \"text\": text}, style=\"ent\", manual=True, options={\"colors\": ner_colors})\n",
    "\n",
    "show_ground_truth_with_spacy(bionlp2004[\"train\"][3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir jetzt wissen, wie unser Ziel-Zustand aussehen soll, wird es Zeit, RoBERTa zu trainieren, um dieses Ziel zu erreichen! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM - Fine-Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Trainer\n",
    "\n",
    "Gestern haben wir zum Training unseres Modells den kompletten Training-Loop, inklusive dem Laden der Trainings-Daten in Batches, dem Berechnen der Loss-Function und der Backpropagation zum Verbessern der Model-Weights, dem Evaluieren der Model-Weights am Validierungs-Split nach jeder Epoche, und dem Zwischenspeichern von Snapshots, \"per Hand\" mit PyTorch implementiert. Da sich nahezu alle dieser Aspekte des Trainings von Modell zu Modell kaum unterscheiden, gibt es als Teil der Hugging Face Transformers Library eine `Trainer` Klasse, die uns den Großteil dieser Arbeit abnimmt. Auch werden die Modelle aus dem Hugging Face Hub, die wir mit Transformers laden, mit der richtigen Loss Function ausgeliefert, wodurch wir diese nicht selbst definieren müssen. Ein paar Vorbereitungen müssen wir jedoch noch treffen, bevor wir `Trainer` verwenden können."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung der Wörter auf Tokens\n",
    "Wie wir bereits gesehen haben, sind unsere Trainings-Daten in Wörter unterteilt. Da unser Model RoBERTa aber mit Sub-Word Tokens arbeitet, müssen wir zuerst unsere Input-Wörter durch RoBERTas Tokenizer schicken, und dann die Tags anpassen, sodass der erste Token eines jeden Wortes den Tag des Wortes zugewiesen bekommt, und alle weiteren Tokens den speziellen Tag `-100` zugeteilt bekommen. Letzteres sorgt dafür, dass diese Tokens in der Loss Funktion ignoriert werden, was hier erwünscht ist, weil sonst längere Wörter einen überproportionalen Einfluss auf den Prediction Loss hätten. Ebenfalls so ignoriert werden Model-spezifische Tokens, die automatisch vom Tokenizer eingefügt werden (wie Start `<s>` und End `</s>` Tokens bei RoBERTa). \n",
    "\n",
    "<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Der spezielle Wert `-100` is spezifisch für die Loss-Funktion des jeweiligen Modells, und z.B. bei RoBERTa [hier](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L926) dokumentiert.\n",
    "\n",
    "<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Versuche, die unten definierte Methode, die einen Batch an conll2003-Einträgen entgegen nimmt, diesen durch RoBERTas Tokenizer schickt, und dann den tokenized Inputs wie oben beschrieben die Labels der ihnen zugehörigen Wörter zuweist. Falls du diesen Schritt überspringen oder deine Lösung kontrollieren willst, führe stattdessen die eingeklappte Notebook-Zelle darunter aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch):\n",
    "    # erhalte die in Tokens unterteilten Einträge des Batches\n",
    "    tokenized_inputs = roberta_tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    # dieser Loop durchwandert je einen in tokenized Eintrag pro Iteration\n",
    "    for i, word_tag_ids in enumerate(batch[f\"tags\"]):\n",
    "        # word_ids enthält für jeden Token den Listen-Index des zugehörigen Wortes im Original-Eintrag, oder None, wenn der Token zu keinem Wort gehört (zB. Start und End Tokens)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        token_tag_ids = []\n",
    "\n",
    "        # befülle token_tag_ids mit einem Eintrag pro Token: \n",
    "        # - die Tag ID des dazugehörigen Wortes (du findest diese in word_tag_ids) für den ersten Token jedes Wortes\n",
    "        # - -100 für weitere Tokens eines Wortes\n",
    "        # - -100 für Tokens die keinem Wort zugehören\n",
    "        \n",
    "        labels.append(token_tag_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = roberta_tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, word_tag_ids in enumerate(batch[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        token_tag_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                token_tag_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                token_tag_ids.append(word_tag_ids[word_idx])\n",
    "            else:\n",
    "                token_tag_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(token_tag_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Transformation müssen wir nun auf den gesamten Datensatz anwenden. Hierzu hat die `Dataset` Klasse eine `map` Methode, mit der Transformationen effizient in Batches durchführbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/jan/.cache/huggingface/datasets/tner___bionlp2004/bionlp2004/1.0.0/9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904/cache-88b61506ffe3a842.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/jan/.cache/huggingface/datasets/tner___bionlp2004/bionlp2004/1.0.0/9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904/cache-67b19b7fa535519e.arrow\n",
      "Loading cached processed dataset at /Users/jan/.cache/huggingface/datasets/tner___bionlp2004/bionlp2004/1.0.0/9f41d3f0270b773c2762dee333ae36c29331e2216114a57081f77639fdb5e904/cache-583acfb8bad7e01f.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_bionlp2004 = bionlp2004.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion zur Berechnung von Metriken\n",
    "Wie gehabt würden wir gerne nach jeder Epoche einen Überblick darüber bekommen, wie unser Modell performt. `Trainer` kann hierzu eine Funktion übergeben werden, die die Ground-Truth Labels und die Predictions des Modells erhält, und daraus beliebige Metriken ableitet. Wir lassen uns die gewohnten Metriken Accuracy, Precision, Recall, und F1 Score wiedergeben, ignorieren aber auch hier jene Tokens, deren Label auf `-100` gesetzt wurde (auch hier um Wörter, die in mehrere Tokens aufgeteilt werden, nicht doppelt zu berücksichtigen).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U evaluate seqeval accelerate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    # Uns werden als Predictions für jeden Token die Gewichte (Logits) für alle möglichen Tag IDs zurück gegeben, daher behandeln wir als eigentliche Prediction des Modells für jeden Token jene Tag ID mit dem höchsten Gewicht\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    filtered_predictions = []\n",
    "    filtered_labels = []\n",
    "\n",
    "    for sample_prediction, sample_label in zip(predictions, labels):\n",
    "        filtered_predictions.append([\n",
    "            id2label[p] for p, l in zip(sample_prediction, sample_label) if l != -100\n",
    "        ])\n",
    "        filtered_labels.append([\n",
    "            id2label[l] for p, l in zip(sample_prediction, sample_label) if l != -100\n",
    "        ])\n",
    "\n",
    "    results = seqeval.compute(predictions=filtered_predictions, references=filtered_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Wir haben die Daten, wir haben die Metriken, unser Modell hat seine Loss-Function bereits eingebaut, somit sind wir startklar! Um `Trainer` zu verwenden, konfigurieren wir es mit einer Instanz von `TrainingArguments`. Diese Config-Klasse enthält diverse bereits bekannte Trainings-Einstellungen wie die Learning-Rate, die Anzahl an Epochs, die trainiert werden soll, und den Pfad, unter dem Snapshots des Modells abgespeichert werden sollen. Nachdem wir ein bereits trainiertes Modell fine-tunen wollen, lassen wir `Trainer` mit einer niedrigen Learning Rate für nur wenige Epochen laufen. \n",
    "\n",
    "Das Training selbst kann dann einfach mit Aufruf der `train()` Methode gestartet werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/jan/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-DNA\",\n",
      "    \"2\": \"I-DNA\",\n",
      "    \"3\": \"B-protein\",\n",
      "    \"4\": \"I-protein\",\n",
      "    \"5\": \"B-cell_type\",\n",
      "    \"6\": \"I-cell_type\",\n",
      "    \"7\": \"B-cell_line\",\n",
      "    \"8\": \"I-cell_line\",\n",
      "    \"9\": \"B-RNA\",\n",
      "    \"10\": \"I-RNA\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DNA\": 1,\n",
      "    \"B-RNA\": 9,\n",
      "    \"B-cell_line\": 7,\n",
      "    \"B-cell_type\": 5,\n",
      "    \"B-protein\": 3,\n",
      "    \"I-DNA\": 2,\n",
      "    \"I-RNA\": 10,\n",
      "    \"I-cell_line\": 8,\n",
      "    \"I-cell_type\": 6,\n",
      "    \"I-protein\": 4,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/jan/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/model.safetensors\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification \n",
    "\n",
    "del roberta_ner\n",
    "roberta_ner = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ").to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/jan/Git/ml-workshop/.venv/lib/python3.11/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16619\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2078\n",
      "  Number of trainable parameters = 124063499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff8cca548ca45579098f8b549f71d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1135, 'learning_rate': 1.5187680461982676e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0711, 'learning_rate': 1.0375360923965351e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1927\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bf97e386794a439ab6a12988281e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../snapshots/roberta-base-token-classification/checkpoint-1039\n",
      "Configuration saved in ../snapshots/roberta-base-token-classification/checkpoint-1039/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0629904568195343, 'eval_precision': 0.7684898520811834, 'eval_recall': 0.8046101206555015, 'eval_f1': 0.7861353039500307, 'eval_accuracy': 0.9456949472690067, 'eval_runtime': 22.6335, 'eval_samples_per_second': 85.139, 'eval_steps_per_second': 5.346, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../snapshots/roberta-base-token-classification/checkpoint-1039/pytorch_model.bin\n",
      "tokenizer config file saved in ../snapshots/roberta-base-token-classification/checkpoint-1039/tokenizer_config.json\n",
      "Special tokens file saved in ../snapshots/roberta-base-token-classification/checkpoint-1039/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.057, 'learning_rate': 5.563041385948027e-06, 'epoch': 1.44}\n",
      "{'loss': 0.0543, 'learning_rate': 7.507218479307027e-07, 'epoch': 1.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tokens, tags. If tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1927\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f19600852d34bb5b13bb9028657ee5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../snapshots/roberta-base-token-classification/checkpoint-2078\n",
      "Configuration saved in ../snapshots/roberta-base-token-classification/checkpoint-2078/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05841284990310669, 'eval_precision': 0.7749451291575216, 'eval_recall': 0.826580226904376, 'eval_f1': 0.7999302892994076, 'eval_accuracy': 0.9489020934991342, 'eval_runtime': 11.4628, 'eval_samples_per_second': 168.109, 'eval_steps_per_second': 10.556, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../snapshots/roberta-base-token-classification/checkpoint-2078/pytorch_model.bin\n",
      "tokenizer config file saved in ../snapshots/roberta-base-token-classification/checkpoint-2078/tokenizer_config.json\n",
      "Special tokens file saved in ../snapshots/roberta-base-token-classification/checkpoint-2078/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../snapshots/roberta-base-token-classification/checkpoint-2078 (score: 0.05841284990310669).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 945.2381, 'train_samples_per_second': 35.164, 'train_steps_per_second': 2.198, 'train_loss': 0.07329441837901904, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2078, training_loss=0.07329441837901904, metrics={'train_runtime': 945.2381, 'train_samples_per_second': 35.164, 'train_steps_per_second': 2.198, 'train_loss': 0.07329441837901904, 'epoch': 2.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "\n",
    "lr = 2e-5\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=roberta_tokenizer)\n",
    "\n",
    "full_training_args = TrainingArguments(\n",
    "    output_dir=\"../snapshots/roberta-base-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True\n",
    ")\n",
    "\n",
    "full_trainer = Trainer(\n",
    "    model=roberta_ner,\n",
    "    args=full_training_args,\n",
    "    train_dataset=tokenized_bionlp2004[\"train\"],\n",
    "    eval_dataset=tokenized_bionlp2004[\"validation\"],\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "full_trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission Accomplished! ... Oder? \n",
    "\n",
    "Unser \"naives\" Fine-Tuning lässt sich mit RoBERTa zwar umsetzen, skaliert aber nicht für Fine-Tuning Projekte mit größeren Modellen:\n",
    "- Mit unserem Ansatz adjustieren wir alle Parameter des Models. `roberta-base` hat ~163 Millionen Parameter, und ist damit nach heutigen Standards ein sehr kleines Modell: Modelle wie LLaMa, FLAN-UL2, Falcon, etc., haben oft 7 Milliarden Parameter in ihrer kleinsten Variante, und größere Varianten mit über 50 Milliarden Parametern. 7 Milliarden Parameter in Float-16 verbrauchen bereits 11-12 GB an Speicher für Inferenz (also ohne jegliche Gradient-Berechnung), somit wäre das Fine-Tuning solcher Modelle auf den meisten handelüblichen Grafikkarten unmöglich, da die zusätzlichen Daten für die Gradient-Berechnung beim Training die 16-24 GB GPU-Memory sprengen würde. \n",
    "- Da wir alle Parameter unseres Modells tunen, müssen wir nun auch alle diese Parameter für jeden unserer Nutzungszwecke ausliefern. Nutzen wir also zB die größte Variante von LLaMa (65B), müssten wir alle 65 Milliarden Parameter für jeden unserer Finetuning-Projekte speichern (über 100 GB).\n",
    "\n",
    "Inzwischen gibt es eine Reihe an Techniken, die unter den Sammelbegriff *Parameter-Efficient Fine-Tuning* fallen, um das Tuning aller Modell-Parameter bei Large Language Models zu vermeiden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter-Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEFT Methoden basieren alle auf dem Prinzip, einem bereits trainierten Modell an unterschiedlichen Stellen neue Parameter anzufügen, und nur diese beim Fine-Tuning zu trainieren. In diese Kategorie fallen:\n",
    "- Soft Prompt Tuning\n",
    "- Prefix Tuning\n",
    "- Adapter\n",
    "- LLaMa Adapter (eine Kombination aus den Prinzipien von Prefix Tuning und Adaptern)\n",
    "- Low-Rank Adaptation (LoRA)\n",
    "\n",
    "Durch ihr Funktionsprinzip mindern PEFT Methoden die oben angesprochenen Probleme mit Fine-Tuning von LLMs: Da nur ein Bruchteil der Model-Parameter optimiert wird, sinkt besonders bei großen Modellen der Speicherbedarf für die Gradient-Berechnung enorm, was das Training auf deutlich schwacherer Hardware möglich macht. Und um unterschiedliche finetuned Varianten des Modells zu erhalten, ist es nur mehr nötig, die Werte der zusätzlichen PEFT Parameter auszutauschen. \n",
    "\n",
    "Im folgenden verwenden wir LoRA, um unser RoBERTa Modell zu tunen. Die technischen Details hinter den einzelnen PEFT-Methoden sind teils etwas kompliziert, glücklicherweise gibt es aber auch hier bereits Libraries, die die Anwendung der Methoden stark vereinfachen. Populär sind unter anderem [Adapter-Transformers](https://github.com/adapter-hub/adapter-transformers) und [Hugging Face PEFT](https://github.com/huggingface/peft). Wir verwenden zweitere, da sich mit der PEFT Library die nötigen Änderungen zu unserem bisherigen Setup darauf beschränken, unser Model mit einem `get_peft_model()` Aufruf zu transformieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1080342 || all params: 355119126 || trainable%: 0.30421960432511314\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config_lora = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n",
    ")\n",
    "\n",
    "roberta_ner_lora = get_peft_model(roberta_ner, peft_config_lora).to(\"mps\")\n",
    "roberta_ner_lora.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie der Output oben zeigt, trainieren wir nun weniger als 0.5% der Parameter des Modells, wodurch die Gradient-Berechnung von PyTorch deutlich vereinfacht wird. Dies äußert sich bei großen Modellen primär im verringerten Speicherverbrauch, kann aber auch zu kürzeren Trainings-Zeiten führen (bei identer Anzahl an Epochen; die AutorInnen des LoRA-Papers beispielsweise bemerkten einen 25% Speedup beim Fine-Tuning von GPT-3 175B). \n",
    "\n",
    "Der restliche Trainings-Code bleibt gleich, jedoch erhöhen wir die Learning-Rate und die Anzahl an Epochen, da wir hier nicht Gefahr laufen, die bereits trainierten Modell-Parameter übermäßig zu verändern. \n",
    "\n",
    "<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Beobachte das Training mittels LoRA im Vergleich zu unserem vorherigen Durchlauf. Läuft das Training schneller/langsamer? Wie sieht der GPU-Speicherverbrauch aus? Ist die Accuracy am Validation-Set merkbar niedriger für das LoRA-Model? \n",
    "\n",
    "<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Inspiziere die Files der beim Training erstellten Snapshots. Wie unterscheiden sie sich von den Snapshots des Fine-Tunings ohne PEFT-Methoden? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"roberta-base-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    use_mps_device=True\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=roberta_ner_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=tokenized_bionlp2004[\"train\"],\n",
    "    eval_dataset=tokenized_bionlp2004[\"validation\"],\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_lora.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferenz \n",
    "Unsere finetuned Modelle können wir nun am Test-Datensatz ausprobieren. Hier laufen wir jedoch in die gleiche Problematik, die wir auch schon im Pre-Processing hatten: Unser Model liefert uns Klassifizierungen auf Token-Ebene, diese müssen wir zum Anzeigen jedoch zuerst wieder auf Wort-Level zusammenfassen. Hierfür gibt es jedoch eine Pipeline, deren Output wir einfach in das Format der `spacy`-Library umwandeln können, um unsere NER-Ergebnisse darzustellen.\n",
    "\n",
    "<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Setze die Umwandlung der Token-Level Outputs des Models in Klassifizierungen auf Wort-Ebene selbst um. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", model=roberta_ner, tokenizer=roberta_tokenizer, aggregation_strategy=\"simple\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'protein',\n",
       "  'score': 0.9465647,\n",
       "  'word': ' glucocorticoid receptors',\n",
       "  'start': 10,\n",
       "  'end': 34},\n",
       " {'entity_group': 'cell_type',\n",
       "  'score': 0.9518865,\n",
       "  'word': ' lymphocytes',\n",
       "  'start': 38,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "test_text = TreebankWordDetokenizer().detokenize(bionlp2004[\"test\"][0][\"tokens\"])\n",
    "token_classifier(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Number of \n",
       "<mark class=\"entity\" style=\"background: #FFA500; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    glucocorticoid receptors\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">protein</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #088F8F; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    lymphocytes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">cell_type</span>\n",
       "</mark>\n",
       " and their sensitivity to hormone action.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from spacy import displacy\n",
    "\n",
    "ner_colors_combined = {\n",
    "    \"DNA\": \"#FF0000\",\n",
    "    \"protein\": \"#FFA500\",\n",
    "    \"cell_type\": \"#088F8F\",\n",
    "    \"cell_line\": \"#CCCCCC\",\n",
    "    \"RNA\": \"#30D5C8\",\n",
    "}\n",
    "\n",
    "def show_result_with_spacy(sample, pipeline):\n",
    "    detokenizer = TreebankWordDetokenizer()\n",
    "    \n",
    "    text = detokenizer.detokenize(sample[\"tokens\"])\n",
    "    result = pipeline(text)\n",
    "    entities = [{\"start\": e[\"start\"], \"end\": e[\"end\"] , \"label\": e[\"entity_group\"]} for e in result]\n",
    "    \n",
    "    displacy.render({\"ents\": entities, \"text\": text}, style=\"ent\", manual=True, options={\"colors\": ner_colors_combined})\n",
    "\n",
    "show_result_with_spacy(bionlp2004[\"test\"][0], token_classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weitere Ressourcen\n",
    "\n",
    "#### Dokumentation\n",
    "- Hugging Face Transformers: https://huggingface.co/docs/transformers/index\n",
    "- Hugging Face Datasets: https://huggingface.co/docs/datasets/index\n",
    "- Hugging Face PEFT: https://huggingface.co/docs/peft/index\n",
    "- Adapter-Transformers (eine Library für die Adapter-Methode für PEFT): https://docs.adapterhub.ml/index.html\n",
    "\n",
    "#### Theorie\n",
    "- Parameter-Efficient Fine-Tuning (PEFT):\n",
    "  - Eine Serie an ausgezeichneten Artikeln über die Intuition hinter den diversen Methoden:\n",
    "    - https://magazine.sebastianraschka.com/p/finetuning-large-language-models\n",
    "    - https://magazine.sebastianraschka.com/p/understanding-parameter-efficient\n",
    "    - https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters\n",
    "    - https://lightning.ai/pages/community/article/understanding-llama-adapters/\n",
    "  - Die ursprünglichen Papers der diversen Methoden:\n",
    "    - Soft Prompt-Tuning: https://arxiv.org/abs/2104.08691\n",
    "    - Prefix Tuning: https://arxiv.org/abs/2101.00190\n",
    "    - Adapters: https://arxiv.org/abs/1902.00751\n",
    "    - LoRa: https://arxiv.org/abs/2106.09685\n",
    "- Transformer:\n",
    "  - Eine weniger technische, dafür intuitive Erklärung der Transformer-Architektur: https://jalammar.github.io/illustrated-transformer/\n",
    "  - Eine sehr technische Zusammenfassung der Transformer Architektur: https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/\n",
    "  - Das originale Transformer Paper mit Code-Umsetzung: http://nlp.seas.harvard.edu/annotated-transformer/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
