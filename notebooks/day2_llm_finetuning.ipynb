{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tag 2 - LLM Fine-Tuning\n\n### Was wir heute lernen werden\nIn den Übungen in diesem Notebook werden wir uns mit Language Models beschäftigen. Wir werden lernen, wie wir Inputs für Language Models pre-processen müssen und wie wir deren Ergebnisse interpretieren können. Zudem werden wir verschiedene Techniken kennenlernen, mit denen wir bestehende Language Models auf unseren konkreten Use-Case anpassen können, von gewöhnlichem Training zu Parameter-Efficient Fine-Tuning (PEFT). Hierfür werden wir diverse Libraries von Hugging Face verwenden, die uns das Laden, das Fine-Tuning, und die Verwendung von öfffentlich verfügbaren ML-Models erleichtern.\n\n### Was wir heute bauen werden\nAm Ende dieser Übungen werden wir ein Language Model zur Token Classification gebaut haben, welches Orte, Personen, und Organisationen in Text erkennen und markieren kann:\n\n<img src=\"https://i.imgur.com/5j5grT4.png\" style=\"width: 800px; height: auto;\" title=\"source: imgur.com\" />\n\n### Vorbereitung\n1. Aktiviere als Accelerator für dein Notebook die Option \"GPU P100\"\n2. Führe die nachfolgenden Notebook-Cells aus, um einige nicht vorinstallierte Libraries zu installieren, und Seeds zu setzen, um reproduzierbare Ergebnisse zu erhalten","metadata":{}},{"cell_type":"code","source":"!pip install evaluate==0.4.0 seqeval==1.2.2 accelerate==0.21.0 peft==0.4.0 gradio","metadata":{"execution":{"iopub.status.busy":"2023-08-19T09:24:53.289604Z","iopub.execute_input":"2023-08-19T09:24:53.290452Z","iopub.status.idle":"2023-08-19T09:25:19.371380Z","shell.execute_reply.started":"2023-08-19T09:24:53.290417Z","shell.execute_reply":"2023-08-19T09:25:19.370132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\n\ntransformers.enable_full_determinism(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T09:25:19.373847Z","iopub.execute_input":"2023-08-19T09:25:19.374226Z","iopub.status.idle":"2023-08-19T09:25:31.035051Z","shell.execute_reply.started":"2023-08-19T09:25:19.374193Z","shell.execute_reply":"2023-08-19T09:25:31.033998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Einführung in das Hugging Face Ökosystem\n\nUm die Bestandteile des ML-Entwicklungsprozesses und die technischen Details hinter ML-Modellen detaillierter zu beleuchten, haben wir gestern reines PyTorch verwendet, um unsere Daten zu laden und aufzubereiten, und unsere Modelle von Grund auf zu bauen, trainieren, und evaluieren. Für die meisten Aufgabenstellungen ist es heute jedoch üblich, bereits vortrainierte Modelle entweder direkt ohne Anpassungen zu verwenden (auch als *zero-shot* bezeichnet), oder diese für die eigene Problemstellung zu finetunen. \n\nZu diesem Zweck hat sich [Hugging Face](https://huggingface.co/) sowohl in der Wissenschaft als auch im kommerziellen Bereich als die primäre Platform herauskristallisiert, um ML-Modelle, Datensätze, etc., auszutauschen. Außerdem stellt Hugging Face eine breite Auswahl an Libraries zur Verfügung, die das Verwenden von bestehenden Modellen und Datensätzen, aber auch Aspekte wie Fine-Tuning, Optimierung für die Nutzung in Production, und Training auf Clustern oder spezieller Hardware (z.B. Google TPUs, Habana Gaudi Prozessoren) vereinfacht. ","metadata":{}},{"cell_type":"markdown","source":"## Hugging Face Transformers\nDie wichtigste Hugging Face Library ist mit Abstand [Transformers](https://huggingface.co/docs/transformers/index). Sie kapselt die Nutzung von vortrainierten Modellen aus verschiedensten Einsatzgebieten, das für sie notwendige Pre- und Post-Processing, sowie ihr Training / Fine-Tuning.  ","metadata":{}},{"cell_type":"markdown","source":"### Basics\nZur Verwendung eines Modells aus dem Hugging Face Ökosystem benötigt man immer mindestens 2 Komponenten: die richtige Model-Klasse, und die Pre-Processors, die die Inputs für die aktuelle Aufgabenstellung (Bilder, Text, etc.) in das Format umwandeln, dass das Modell erwartet. Laden wir ein Beispiel-Modell, das Language Model RoBERTa (https://huggingface.co/roberta-base):","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, RobertaForMaskedLM\n\nroberta_mlm = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\nroberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T09:28:08.915853Z","iopub.execute_input":"2023-08-19T09:28:08.916702Z","iopub.status.idle":"2023-08-19T09:28:23.405869Z","shell.execute_reply.started":"2023-08-19T09:28:08.916664Z","shell.execute_reply":"2023-08-19T09:28:23.404641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Die vorangehenden Zeilen laden das RoBERTa Modell, und seinen Pre-Processor, den Tokenizer: Da ein Language Model keine Strings, sondern Zahlen als Input erwartet, muss ein Eingabe-String zuerst in kleinere Teile aufgeteilt werden, die *Tokens*. Je nach Modell kommen hier unterschiedliche Techniken zum Einsatz, bei denen ein Token immer ein gesamtes Wort, oder häufiger den Teil eines Wortes repräsentiert (Sub-Word Tokenization). Die Verwendung der `from_pretrained` Methode mit dem selben Model-Identifier stellt sicher, dass wir die richtige Kombination aus Modell und Tokenizer laden.\n\nRoBERTa gehört der Familie der Encoder Transformer Modelle an, die schematisch wie folgt aufgebaut sind: \n\n<img src=\"https://i.imgur.com/GpBw39b.png\" style=\"width: 800px; height: auto;\" title=\"source: imgur.com\" />\n\nEin Encoder Transformer nimmt als Input einen in Tokens umgewandelten String, und konvertiert jeden Token in einen sogenannten Embedding-Vector (im Falle von RoBERTa sind diese 768-dimensional). Diese Vektoren werden dann alle gleichzeitig durch mehrere Encoder-Blöcke geschickt, und vom letzten Block werden wieder 768-dimensionale Output-Vektoren zurückgegeben. Das Model akzeptiert dabei immer eine fixe Anzahl an Input-Vektoren (512 bei RoBERTa). Wenn der Input-String in weniger als 512 Tokens aufgeteilt wird, werden die verbleibenden Stellen einfach mit einem speziellen Padding-Token aufgefüllt.   \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Der Tokenizer von RoBERTa ist ein Byte Pair Encoding (BPE) Tokenizer. Diese Technik wurde erstmals von OpenAI GPT-2 verwendet, und wird hier gut erläutert: https://lucytalksdata.com/the-modern-tokenization-stack-for-nlp-byte-pair-encoding/. \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Der Prozess von Tokenization zum Embedding wird in diesem Artikel sehr umfangreich erklärt: https://www.lesswrong.com/posts/pHPmMGEMYefk9jLeh/llm-basics-embedding-spaces-transformer-token-vectors-are. \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Die Encoder-Blöcke nutzen einen Prozess namens \"Multi-Headed Self-Attention\", der aus der ursprünglichen Transformer-Architektur kommt, aus der sich nahezu alle modernen Language Models ableiten. Dieser Prozess wird hier gut erläutert: http://jalammar.github.io/illustrated-transformer/, aber die Quintessenz ist, dass in jedem Encoder-Block für jeden Embedding-Vektor die Information aller anderen Embedding-Vektoren (also jene davor **und** danach) verwendet wird, um den Vektor zu ändern. ","metadata":{}},{"cell_type":"markdown","source":"Wie verwenden wir nun unser Modell? Wie du aus dem Code oben erkennen kannst, haben wir eine Variante von RoBERTa geladen, die für Masked Language Modelling (MLM) finetuned wurde, also das Füllen von Lücken in einem Text. Sehen wir uns ein Beispiel an:","metadata":{}},{"cell_type":"code","source":"import torch \n\ninputs = roberta_tokenizer(\"The capital of France is <mask>.\", return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits = roberta_mlm(**inputs).logits\n\nmask_token_index = (inputs.input_ids == roberta_tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\npredicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\nroberta_tokenizer.decode(predicted_token_id)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T09:33:19.059469Z","iopub.execute_input":"2023-08-19T09:33:19.060743Z","iopub.status.idle":"2023-08-19T09:33:19.372380Z","shell.execute_reply.started":"2023-08-19T09:33:19.060688Z","shell.execute_reply":"2023-08-19T09:33:19.371440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wie erwartet leiten wir unseren Input-Satz durch den Tokenizer, und geben das Ergebnis des Tokenizers 1-zu-1 and unser Modell weiter. Das Modell gibt für den maskierten Token (in diesem Fall enthält der Satz nur einen) Gewichte zurück (sogenannte Logits), die ausdrücken, welchen Token das Modell an dieser Stelle am wahrscheinlichsten einsetzen würde. Wie erwartet füllt das Modell die Lücke im Text mit der korrekten Antwort: Paris. \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Wie entstehen die Logits für die maskierten Tokens? Der oben illustrierten Struktur des Encoder Transformers werden einige Fully-Connected Layer hinzugefügt, durch die das Output Embedding jedes Tokens geleitet wird. Am Ende befindet sich ein Layer mit genau so vielen Neuronen, wie Tokens im Vokabular des Models, und einer Softmax Activation Function. Für jeden maskierten Token haben wir hier also quasi ein Klassifizierungs-Problem, wobei die Klassen alle möglichen Tokens sind, die die Lücke im Text füllen könnten. \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Sieh dir die `inputs` an das Modell genauer an. Stimmt die Anzahl an Tokens (`input_ids`) mit der Anzahl an Wörtern überein, und wenn nicht, was könnte die Ursache sein (um eine Vermutung zu entkräftigen, sieh dir den Output von `inputs.word_ids(batch_index=0)` an)? Was könnte der Zweck der `attention_mask` sein? \n\n<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Leite dir ab, wie der Code oben genau funktioniert, um an die Antwort des Modells zu kommen. Tipps: \n- Das Modell verarbeitet gewöhnlich nicht nur einen, sondern einen Batch an Sätzen. Wenn `roberta_tokenizer` mit einer Liste aufgerufen wird, bekommst du einen Tensor zurück, in dem jede Zeile die Tokens für einen Eintrag in der Liste enthält. \n- `tokenizer.mask_token_id` enthält die ID des Tokens, in den `<mask>` umgewandelt wird (also die ID, an der wir interessiert sind). \n- Ein Vergleich mit == zwischen einem Tensor und einem Integer vergleich jedes Element im Tensor mit diesem Integer und gibt einen Boolean-Tensor im selben Format zurück, der die Ergebnisse dieser Vergleiche enthält. \n- `nonzero(as_tuple=True)` gibt dir zwei Tensoren zurück: einmal die Zeilen und einmal die Spalten, in denen sich im Tensor ein Wert `!= 0` (also `!= False`) befindet. Ist der Output z.B. `[0,1]` und `[6, 3]`, befinden sich im Tensor in Zeile 0 und Spalte 6, sowie in Zeile 1 und Spalte 3 der Wert `1/True`, das sind also die Positionen unserer Masks.\n- Ein mehr-dimensionaler Tensor kann mit ein-dimensionalen Tensoren indiziert werden: `logits[[0,1], [6,3]]` gibt die Werte in Zeile 0 und Spalte 6, sowie in Zeile 1 und Spalte 3 von einem `logits` Tensor zurück.\n- `logits` enthält hier Werte für alle möglichen Tokens im Vokabular des LLMs. Die Werte können unterschiedliche Dinge ausdrücken (https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), haben aber in jedem Fall eine ähnliche Bedeutung wie die Wahrscheinlichkeiten aus einem Softmax Layer: je höher der Wert für einen Eintrag, desto wahrscheinlicher will das ML Model diesen auswählen. Mit `argmax()` holen wir uns also jenen Token aus dem Vokabular des Models, der nach Einschätzung des Models am besten den `<mask>` Token ersetzen soll. \n\nBei guten Verständnis solltest du den Code so umbauen können, dass du die Antworten für einen Batch an Sätzen auf einmal auslesen kannst. \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Transformers stellt sogenannte *Pipelines* zur Verfügung, die das Post-Processing der Model-Outputs übernehmen. Nutze die Pipelines API-Doc (https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/pipelines) um das gleiche Ergebnis zu erhalten. Übergib der Pipeline dabei die bereits initialisierten Modell & Tokenizer.","metadata":{}},{"cell_type":"code","source":"# Aufgabe 1:\n# Todo: Kommentiere einzelne der unteren Zeilen aus und führe diese Notebook Cell aus, um dir die Model Inputs genauer anzuschauen\n\n# inputs \n\n# inputs.word_ids(batch_index=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bonus-Aufgabe:\n# padding=True ist nötig, um die Tokenization der Input Strings auf die selbe Länge aufzufüllen,\n# damit sie als ein Tensor dargestellt werden können\ninputs_batch = roberta_tokenizer([\"The capital of France is <mask>.\", \"The city <mask> is the capital of Germany.\"], padding=True, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    logits_batch = roberta_mlm(**inputs_batch).logits\n        \n# Todo: Ergänze hier eine Abwandlung obrigen Codes, um die ausgefüllten Masken für alle Sätze im Batch auszugeben\n# Referenz-Lösung: https://gist.github.com/winnedatsch/9698950aaa1e7855ae79cc5d90e58674","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:01:33.294703Z","iopub.execute_input":"2023-08-19T10:01:33.295315Z","iopub.status.idle":"2023-08-19T10:01:33.543516Z","shell.execute_reply.started":"2023-08-19T10:01:33.295266Z","shell.execute_reply":"2023-08-19T10:01:33.542125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aufgabe 2:\nfrom transformers import pipeline\n\n# Todo: Nutze eine Transformers-Pipeline, um das selbe Ergebnis zu erhalten","metadata":{"execution":{"iopub.status.busy":"2023-08-13T20:29:53.607565Z","iopub.execute_input":"2023-08-13T20:29:53.607937Z","iopub.status.idle":"2023-08-13T20:29:55.744935Z","shell.execute_reply.started":"2023-08-13T20:29:53.607908Z","shell.execute_reply":"2023-08-13T20:29:55.743969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Weitere Details zum Laden von Modellen\n\nWie bereits aus dem `with torch.no_grad()` Befehl erkennbar sein sollte, ist das mittels Transformers geladene Modell ein reguläres PyTorch Modell. Wir können uns daher die Details des Modells wie gewohnt ansehen, und auch sonst ohne die Helfer von Transformers mit dem Modell so interagieren, wie wir es von einem PyTorch Modell gewohnt sind:","metadata":{}},{"cell_type":"code","source":"from torchinfo import summary\n\nsummary(roberta_mlm)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:12:14.654063Z","iopub.execute_input":"2023-08-19T10:12:14.654468Z","iopub.status.idle":"2023-08-19T10:12:14.711484Z","shell.execute_reply.started":"2023-08-19T10:12:14.654440Z","shell.execute_reply":"2023-08-19T10:12:14.710530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Diese Model-Summary deutet noch auf ein weiteres Feature von Transformers hin: Viele Modelle wurden nicht nur für einen, sondern mehrere Aufgaben vorbereitet. In unserem Fall sehen wir, dass unser `RobertaForMaskedLM` Modell 2 Bestandteile hat: Das Kern-RoBERTa Modell (`RobertaModel`) und eine Gruppe an zusätzlichen Layern, die den rohen Output des `RobertaEncoder`s in die Logit-Gewichte umwandelt, die wir oben gesehen haben (`RobertaLMHead`). Im Falle von RoBERTa wurden auch *Heads* für andere Aufgabenstellungen spezifiziert, unter anderem für jenen Task, mit dem wir uns als nächstes beschäftigen werden: Named Entity Recognition (NER). \n\nUm das Modell mit einem Head für NER zu laden, könnten wir die `RobertaForTokenClassification`-Klasse verwenden. Da Transformers die Interfaces der Modelle für die gleiche Aufgabenstellung standardisiert, können wir stattdessen eine der sogenannten `Auto`-Klassen verwenden, die die Details zum Modell automatisch aus seiner Spezifikation lädt:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification\n\nroberta_ner = AutoModelForTokenClassification.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:13:34.294098Z","iopub.execute_input":"2023-08-19T10:13:34.294556Z","iopub.status.idle":"2023-08-19T10:13:36.081171Z","shell.execute_reply.started":"2023-08-19T10:13:34.294527Z","shell.execute_reply":"2023-08-19T10:13:36.080133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wie wir an der Warnung erkennen, wurde zwar das Format für einen NER Head für RoBERTa spezifiziert, der Checkpoint `roberta-base` enthält jedoch keine Weights für diesen Head, weshalb wir ihn erst für unseren spezifischen Einsatzzweck trainieren müssen.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background: #dd11d7; font-size: 16px; padding: 10px; border-radius: 1px; color: white; text-align: center; font-weight: bold;\">Ende Übung 1</div>","metadata":{}},{"cell_type":"markdown","source":"## Named Entity Recognition (NER)\n\nNach unserem etwas akademischen Beispiel widmen wir uns jetzt einem praktischeren Language-Model Task: Named Entity Recognition, oft auch Token Classification genannt. Hier sollen in einem unstrukturierten Text bestimmte Entitäten (Firmen, Personen, Orte, etc.) erkannt und extrahiert werden. Beispiele für Nutzungs-Szenarien sind:\n\n- Automatische Zuordnung von Support-Anfragen basierend auf den erkannten Regionen, Produkten, Herstellern, etc.\n- Automatisches Anreichern mit Kontext-Informationen, z.B. die aktuellen Aktienkurse für erkannte Organisationen in Zeitungs-Artikeln.\n- Intelligente Suchfunktionen, die beispielsweise eine medizinische Diagnose-Datenbank nach einer bestimmten Krankheits-Kategorie filtern.\n","metadata":{}},{"cell_type":"markdown","source":"### Hugging Face Datasets\n\nAls Datensatz für unsere folgenden Experimente verwenden wir `wikiann` (https://huggingface.co/datasets/wikiann), eine Sammlung Wikipedia-Artikeln, in denen Orte (LOC), Personen (PER), und Organisationen (ORG) hervorgehoben sind. Ein konkreter Use-Case eines Unternehmens wird mit hoher Wahrscheinlichkeit auch einen darauf angepassten Datensatz benötigen, für die Erklärung der technischen Prinzipien ist `wikiann` jedoch ausreichend. \n\nÄhnlich wie Hugging Face Transformers für Modelle gibt Hugging Face Datasets dem Entwickler die Möglichkeit, bekannte Referenz-Datensätze über ein simpel gestaltetes Interface zu laden:","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nwikiann = load_dataset(\"wikiann\", \"en\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:17:06.413883Z","iopub.execute_input":"2023-08-19T10:17:06.414307Z","iopub.status.idle":"2023-08-19T10:17:40.830748Z","shell.execute_reply.started":"2023-08-19T10:17:06.414276Z","shell.execute_reply":"2023-08-19T10:17:40.829661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wikiann","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:17:52.947884Z","iopub.execute_input":"2023-08-19T10:17:52.948900Z","iopub.status.idle":"2023-08-19T10:17:52.955532Z","shell.execute_reply.started":"2023-08-19T10:17:52.948863Z","shell.execute_reply":"2023-08-19T10:17:52.954580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sehen wir uns den geladenen Datensatz etwas genauer an. Der Datensatz wird bereits beim Laden in Train, Validation, und Test Splits aufgeteilt. Jeder dieser Splits ist eine Instanz der `Dataset` Klasse, die effizientes Pre-Processing der enthaltenen Daten ermöglicht und ausgezeichnet mit den restlichen Hugging Face Libraries (wie z.B. Transformers) zusammen arbeitet. \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Falls man die Features der Datasets Library auch mit eigenen Daten verwenden möchte, gibt es eine Reihe an Möglichkeiten, um ein Dataset zu konstruieren: z.B. aus [diversen Datei-Formaten](https://huggingface.co/docs/datasets/v2.13.1/en/package_reference/loading_methods#from-files), aus [Pandas `DataFrame`s](https://huggingface.co/docs/datasets/v2.13.1/en/package_reference/main_classes#datasets.Dataset.from_pandas), und aus [PyTorch `Dataset`s](https://github.com/huggingface/datasets/issues/4983).\n\n","metadata":{}},{"cell_type":"code","source":"print(wikiann[\"train\"][19])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:18:30.174394Z","iopub.execute_input":"2023-08-19T10:18:30.174848Z","iopub.status.idle":"2023-08-19T10:18:30.182985Z","shell.execute_reply.started":"2023-08-19T10:18:30.174815Z","shell.execute_reply":"2023-08-19T10:18:30.181867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Die einzelnen Einträge des Datensatzes enthalten eine Reihe an `\"tokens\"` (wobei hier `\"tokens\"` einfach die einzelnen Worte des jeweiligen Texts beschreibt, nicht zu verwechseln mit den numerischen Tokens, die vom Tokenizer unseres Modells produziert werden), und dazugehörige `\"ner_tags\"`, die jedem `token` eine Entitäts-Klasse zuordnen (oder 0, falls keine zutrifft). Um die numerischen Tag-Werte ihren Labeln zuordnen zu können, bauen wir uns Maps in beide Richtungen.\n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Falls im eigenen Datensatz die Input-Texte noch nicht in Wörter unterteilt sind, kann dies beispielsweise mit dem `TreebankWordTokenzier` aus der NLTK Library erreicht werden: https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html. \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Unser Modell, RoBERTa, verwendet wie bereits erwähnt Sub-Word Tokenization. Welche Probleme siehst du darin, ein solches Modell für einen Einsatzzweck zu nutzen, in dem immer ganze Wörter oder Wortgruppen kategorisiert werden müssen? ","metadata":{}},{"cell_type":"code","source":"label2id = {label: i for i, label in enumerate(wikiann[\"train\"].features[f\"ner_tags\"].feature.names)}\nid2label = {i: label for label, i in label2id.items()}\n\nlabel2id","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:19:31.843151Z","iopub.execute_input":"2023-08-19T10:19:31.843537Z","iopub.status.idle":"2023-08-19T10:19:31.852165Z","shell.execute_reply.started":"2023-08-19T10:19:31.843508Z","shell.execute_reply":"2023-08-19T10:19:31.851034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Damit wir uns die Aufgabenstellung besser vorstellen können, nutzen wir die Library `spacy` um die Entitäten in einem Beispielsatz hervorzuheben.\n\n<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Die Token-Klassen folgen dem [IOB2-Schema](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), in dem bei zusammengehörigen Wörtern das erste mit \"B-\" Prefix, und alle zugehörigen Folgewörter mit \"I-\" Prefix klassifiziert werden. Ändere den folgenden Code so ab, dass zusammengehörige Wörter als einzelne Entität angezeigt werden.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom spacy import displacy\n\nner_colors = {\n    \"B-PER\": \"#FF0000\",\n    \"I-PER\": \"#FF0000\",\n    \"B-ORG\": \"#FFA500\",\n    \"I-ORG\": \"#FFA500\",\n    \"B-LOC\": \"#088F8F\",\n    \"I-LOC\": \"#088F8F\"\n}\n\ndef show_ground_truth_with_spacy(sample):\n    detokenizer = TreebankWordDetokenizer()\n    text = detokenizer.detokenize(sample[\"tokens\"])\n    entities = []\n    current_start = 0\n    \n    for word, tag in zip(sample[\"tokens\"], sample[\"ner_tags\"]):\n        if tag != 0:\n            word_start = text.find(word, current_start)\n            word_end = word_start + len(word)\n            entities.append({\"label\": id2label[tag], \"start\": word_start, \"end\": word_end})\n            current_start = word_end\n\n    displacy.render({\"ents\": entities, \"text\": text}, style=\"ent\", manual=True, options={\"colors\": ner_colors})\n\nshow_ground_truth_with_spacy(wikiann[\"train\"][19])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:19:44.766255Z","iopub.execute_input":"2023-08-19T10:19:44.766644Z","iopub.status.idle":"2023-08-19T10:19:47.999572Z","shell.execute_reply.started":"2023-08-19T10:19:44.766606Z","shell.execute_reply":"2023-08-19T10:19:47.998525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nachdem wir jetzt wissen, wie unser Ziel-Zustand aussehen soll, wird es Zeit, RoBERTa zu trainieren, um dieses Ziel zu erreichen! ","metadata":{}},{"cell_type":"markdown","source":"## LLM - Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"### Transformers Trainer\n\nGestern haben wir zum Training unseres Modells den kompletten Training-Loop, inklusive dem Laden der Trainings-Daten in Batches, dem Berechnen der Loss-Function und der Backpropagation zum Verbessern der Model-Weights, dem Evaluieren der Model-Weights am Validierungs-Split nach jeder Epoche, und dem Zwischenspeichern von Snapshots, \"per Hand\" mit PyTorch implementiert. Da sich nahezu alle diese Aspekte des Trainings von Modell zu Modell kaum unterscheiden, gibt es als Teil der Hugging Face Transformers Library eine `Trainer` Klasse, die uns den Großteil dieser Arbeit abnimmt. Auch werden die Modelle aus dem Hugging Face Hub, die wir mit Transformers laden, mit der richtigen Loss Function ausgeliefert, wodurch wir diese nicht selbst definieren müssen. Ein paar Vorbereitungen müssen wir jedoch noch treffen, bevor wir `Trainer` verwenden können.","metadata":{}},{"cell_type":"markdown","source":"### Aufteilung der Wörter auf Tokens\nWie wir bereits gesehen haben, sind unsere Trainings-Daten in Wörter unterteilt. Da unser Model RoBERTa aber mit Sub-Word Tokens arbeitet, müssen wir zuerst unsere Input-Wörter durch RoBERTas Tokenizer schicken, und dann die Tags anpassen, sodass der erste Token jedes Wortes den Tag des Wortes zugewiesen bekommt, und alle weiteren Tokens den speziellen Tag `-100` zugeteilt bekommen. Letzteres sorgt dafür, dass diese Tokens in der Loss Funktion ignoriert werden, was hier erwünscht ist, weil sonst längere Wörter einen überproportionalen Einfluss auf den Prediction Loss hätten. Ebenfalls so ignoriert werden Model-spezifische Tokens, die automatisch vom Tokenizer eingefügt werden (wie Start `<s>` und End `</s>` Tokens bei RoBERTa). \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Der spezielle Wert `-100` is spezifisch für die Loss-Funktion des jeweiligen Modells, und z.B. bei RoBERTa [hier](https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L926) dokumentiert.\n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Versuche, die unten definierte Methode fertig zu implementieren, die einen Batch an `wikiann`-Einträgen entgegen nimmt, diesen durch RoBERTas Tokenizer schickt, und dann den tokenized Inputs wie oben beschrieben die Tags der ihnen zugehörigen Wörter zuweist. Falls du diesen Schritt überspringen oder deine Lösung kontrollieren willst, nimm dir die Refernzlösung aus folgendem Gist: https://gist.github.com/winnedatsch/21eaebd195956626a84d0a176c74f487. ","metadata":{}},{"cell_type":"code","source":"def tokenize_and_align_labels(batch):\n    # erhalte die in Tokens unterteilten Samples des Batches\n    tokenized_inputs = roberta_tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n\n    labels = []\n    # dieser Loop durchwandert je ein Sample pro Iteration\n    # word_tag_ids enthält für das aktuelle Sample die NER Tags für jedes Wort\n    for i, word_tag_ids in enumerate(batch[f\"ner_tags\"]):\n        # word_ids enthält für jeden Token den Listen-Index des zugehörigen Wortes im Original-Eintrag, oder None, wenn der Token zu keinem Wort gehört (zB. Start und End Tokens)\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        \n        \n        token_tag_ids = []\n        # befülle token_tag_ids mit einem Eintrag pro Token: \n        # - die Tag ID des dazugehörigen Wortes (du findest diese in word_tag_ids) für den ersten Token jedes Wortes\n        # - -100 für weitere Tokens eines Wortes (also jene Tokens, deren Eintrag in word_ids gleich wie der des vorhergehenden Tokens ist)\n        # - -100 für Tokens die keinem Wort zugehören (also jene Tokens, deren Eintrag in word_ids None ist)\n        \n        labels.append(token_tag_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-08-13T20:31:05.226343Z","iopub.execute_input":"2023-08-13T20:31:05.226710Z","iopub.status.idle":"2023-08-13T20:31:05.233039Z","shell.execute_reply.started":"2023-08-13T20:31:05.226680Z","shell.execute_reply":"2023-08-13T20:31:05.232139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(batch):\n    tokenized_inputs = roberta_tokenizer(batch[\"tokens\"], truncation=True, is_split_into_words=True)\n\n    labels = []\n    for i, word_tag_ids in enumerate(batch[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        token_tag_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                token_tag_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                token_tag_ids.append(word_tag_ids[word_idx])\n            else:\n                token_tag_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(token_tag_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:46:53.551609Z","iopub.execute_input":"2023-08-19T10:46:53.552020Z","iopub.status.idle":"2023-08-19T10:46:53.560632Z","shell.execute_reply.started":"2023-08-19T10:46:53.551989Z","shell.execute_reply":"2023-08-19T10:46:53.559323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Diese Transformation müssen wir nun auf den gesamten Datensatz anwenden. Hierzu hat die `Dataset` Klasse eine `map` Methode, mit der Transformationen effizient in Batches durchführbar sind.","metadata":{}},{"cell_type":"code","source":"tokenized_wikiann = wikiann.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:46:55.879032Z","iopub.execute_input":"2023-08-19T10:46:55.879415Z","iopub.status.idle":"2023-08-19T10:47:02.822056Z","shell.execute_reply.started":"2023-08-19T10:46:55.879387Z","shell.execute_reply":"2023-08-19T10:47:02.821028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Funktion zur Berechnung von Metriken\nWie gehabt würden wir gerne nach jeder Epoche einen Überblick darüber bekommen, wie unser Modell performt. `Trainer` kann hierzu eine Funktion übergeben werden, die die Ground-Truth Labels und die Predictions des Modells erhält, und daraus beliebige Metriken ableitet. Wir lassen uns die gewohnte Metrik Accuracy, aber auch die zusätzichen Metriken Precision, Recall, und F1 Score wiedergeben. Wir ignorieren auch hier jene Tokens, deren Label auf `-100` gesetzt wurde (auch hier um Wörter, die in mehrere Tokens aufgeteilt werden, nicht doppelt zu berücksichtigen). \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Eine Erklärung zu Precision und Recall findest du hier: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\nseqeval = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    # Labels: Korrekte Tag-Zuordnung, die unsere tokenize_and_align_labels Funktion erstellt hat\n    # Predictions: Antworten unseres Models\n    predictions, labels = p\n    \n    # Uns werden als Predictions für jeden Token die Gewichte (Logits) für \n    # alle möglichen Tag IDs zurück gegeben, daher behandeln wir als eigentliche \n    # Prediction des Modells für jeden Token jene Tag ID mit dem höchsten Gewicht\n    predictions = np.argmax(predictions, axis=2)\n\n    filtered_predictions = []\n    filtered_labels = []\n\n    # Da wir einen Batch an Predictions und Labels bekommen, behandeln wir ein Sample nach dem anderen\n    for sample_prediction, sample_label in zip(predictions, labels):\n        # Für jedes Sample filtern wir aus den Labels und den Predictions die Tokens heraus, \n        # die in den Labels die Tag ID -100 zugewiesen bekommen haben\n        filtered_predictions.append([\n            id2label[p] for p, l in zip(sample_prediction, sample_label) if l != -100\n        ])\n        filtered_labels.append([\n            id2label[l] for p, l in zip(sample_prediction, sample_label) if l != -100\n        ])\n\n    results = seqeval.compute(predictions=filtered_predictions, references=filtered_labels)\n    \n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:54:37.060072Z","iopub.execute_input":"2023-08-19T10:54:37.060499Z","iopub.status.idle":"2023-08-19T10:54:40.198787Z","shell.execute_reply.started":"2023-08-19T10:54:37.060468Z","shell.execute_reply":"2023-08-19T10:54:40.197790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\nWir haben die Daten, wir haben die Metriken, unser Modell hat seine Loss-Function bereits eingebaut, somit sind wir startklar! Um `Trainer` zu verwenden, konfigurieren wir es mit einer Instanz von `TrainingArguments`. Diese Config-Klasse enthält diverse bereits bekannte Trainings-Einstellungen wie die Learning-Rate, die Anzahl an Epochs, die trainiert werden soll, und den Pfad, unter dem Snapshots des Modells abgespeichert werden sollen. Nachdem wir ein bereits trainiertes Modell fine-tunen wollen, lassen wir `Trainer` mit einer niedrigen Learning Rate für nur wenige Epochen laufen. \n\nDas Training selbst kann dann einfach mit Aufruf der `train()` Methode gestartet werden. ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification \n\ndel roberta_ner\nroberta_ner = AutoModelForTokenClassification.from_pretrained(\n    \"roberta-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id\n).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:55:19.208950Z","iopub.execute_input":"2023-08-19T10:55:19.209359Z","iopub.status.idle":"2023-08-19T10:55:26.906536Z","shell.execute_reply.started":"2023-08-19T10:55:19.209327Z","shell.execute_reply":"2023-08-19T10:55:26.905427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForTokenClassification\n\nlr = 2e-5\nbatch_size = 16\nnum_epochs = 2\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=roberta_tokenizer)\n\nfull_training_args = TrainingArguments(\n    output_dir=\"../snapshots/roberta-base-token-classification\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n\nfull_trainer = Trainer(\n    model=roberta_ner,\n    args=full_training_args,\n    train_dataset=tokenized_wikiann[\"train\"],\n    eval_dataset=tokenized_wikiann[\"validation\"],\n    tokenizer=roberta_tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nfull_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T10:55:30.339325Z","iopub.execute_input":"2023-08-19T10:55:30.339698Z","iopub.status.idle":"2023-08-19T11:05:25.249104Z","shell.execute_reply.started":"2023-08-19T10:55:30.339666Z","shell.execute_reply":"2023-08-19T11:05:25.247451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mission Accomplished! ... Oder? \n\nUnser \"naives\" Fine-Tuning lässt sich mit RoBERTa zwar umsetzen, skaliert aber nicht für Fine-Tuning Projekte mit größeren Modellen:\n- Mit unserem Ansatz adjustieren wir alle Parameter des Models. `roberta-base` hat ~163 Millionen Parameter, und ist damit nach heutigen Standards ein sehr kleines Modell: Modelle wie LLaMa, FLAN-UL2, Falcon, etc., haben oft 7 Milliarden Parameter in ihrer kleinsten Variante, und größere Varianten mit über 50 Milliarden Parametern. 7 Milliarden Parameter in Float-16 verbrauchen bereits 11-12 GB an Speicher für Inferenz (also ohne jegliche Gradient-Berechnung), somit wäre das Fine-Tuning solcher Modelle auf den meisten handelüblichen Grafikkarten unmöglich, da die zusätzlichen Daten für die Gradient-Berechnung beim Training die 16-24 GB GPU-Memory sprengen würde. \n- Da wir alle Parameter unseres Modells tunen, müssen wir nun auch alle diese Parameter für jeden unserer Nutzungszwecke ausliefern. Nutzen wir also zB die größte Variante von LLaMa (65B), müssten wir alle 65 Milliarden Parameter für jedes unserer Finetuning-Projekte speichern (über 100 GB).\n\nInzwischen gibt es eine Reihe an Techniken, die unter den Sammelbegriff *Parameter-Efficient Fine-Tuning* fallen, um das Tuning aller Modell-Parameter bei Large Language Models zu vermeiden.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background: #dd11d7; font-size: 16px; padding: 10px; border-radius: 1px; color: white; text-align: center; font-weight: bold;\">Ende Übung 2</div>","metadata":{}},{"cell_type":"markdown","source":"## Parameter-Efficient Fine-Tuning (PEFT)","metadata":{}},{"cell_type":"markdown","source":"PEFT Methoden basieren alle auf dem Prinzip, einem bereits trainierten Modell an unterschiedlichen Stellen neue Parameter anzufügen, und nur diese beim Fine-Tuning zu trainieren. In diese Kategorie fallen:\n- Soft Prompt Tuning\n- Prefix Tuning\n- Adapter\n- LLaMa Adapter (eine Kombination aus den Prinzipien von Prefix Tuning und Adaptern)\n- Low-Rank Adaptation (LoRA)\n\nDurch ihr Funktionsprinzip mindern PEFT Methoden die oben angesprochenen Probleme mit Fine-Tuning von LLMs: Da nur ein Bruchteil der Model-Parameter optimiert wird, sinkt besonders bei großen Modellen der Speicherbedarf für die Gradient-Berechnung enorm, was das Training auf deutlich schwacherer Hardware möglich macht. Und um unterschiedliche finetuned Varianten des Modells zu erhalten, ist es nur mehr nötig, die Werte der zusätzlichen PEFT Parameter auszutauschen. \n\nIm folgenden verwenden wir LoRA, um unser RoBERTa Modell zu tunen. Die technischen Details hinter den einzelnen PEFT-Methoden sind teils etwas kompliziert, glücklicherweise gibt es aber auch hier bereits Libraries, die die Anwendung der Methoden stark vereinfachen. Populär sind unter anderem [Adapter-Transformers](https://github.com/adapter-hub/adapter-transformers) und [Hugging Face PEFT](https://github.com/huggingface/peft). Wir verwenden zweitere, da sich mit der PEFT Library die nötigen Änderungen zu unserem bisherigen Setup darauf beschränken, unser Model mit einem `get_peft_model()` Aufruf zu transformieren:","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\npeft_config_lora = LoraConfig(\n    task_type=TaskType.TOKEN_CLS, inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1, bias=\"all\"\n)\n\nroberta_ner_lora = AutoModelForTokenClassification.from_pretrained(\n    \"roberta-base\", num_labels=len(id2label), id2label=id2label, label2id=label2id\n).to(\"cuda\")\nroberta_ner_lora = get_peft_model(roberta_ner_lora, peft_config_lora).to(\"cuda\")\nroberta_ner_lora.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:07:02.759251Z","iopub.execute_input":"2023-08-19T11:07:02.759710Z","iopub.status.idle":"2023-08-19T11:07:05.281659Z","shell.execute_reply.started":"2023-08-19T11:07:02.759670Z","shell.execute_reply":"2023-08-19T11:07:05.280543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wie der Output oben zeigt, trainieren wir nun weniger als 0.5% der Parameter des Modells, wodurch die Gradient-Berechnung von PyTorch deutlich vereinfacht wird. Dies äußert sich bei großen Modellen primär im verringerten Speicherverbrauch, kann aber auch zu kürzeren Trainings-Zeiten führen (bei identer Anzahl an Epochen; die AutorInnen des LoRA-Papers beispielsweise bemerkten einen 25% Speedup beim Fine-Tuning von GPT-3 175B). \n\nDer restliche Trainings-Code bleibt gleich, jedoch erhöhen wir die Learning-Rate und die Anzahl an Epochen, da wir hier nicht Gefahr laufen, die bereits trainierten Modell-Parameter übermäßig zu verändern. \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Beobachte das Training mittels LoRA im Vergleich zu unserem vorherigen Durchlauf. Ist die Accuracy am Validation-Set merkbar niedriger für das LoRA-Model? \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Inspiziere die Files der beim Training erstellten Snapshots (diese liegen in `/kaggle/snapshots/`). Wie unterscheiden sie sich von den Snapshots des Fine-Tunings ohne PEFT-Methoden? ","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nlr = 1e-3\nbatch_size = 16\nnum_epochs = 4\n\ntraining_args_lora = TrainingArguments(\n    output_dir=\"roberta-base-lora-token-classification\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"none\"\n)\n\ntrainer_lora = Trainer(\n    model=roberta_ner_lora,\n    args=training_args_lora,\n    train_dataset=tokenized_wikiann[\"train\"],\n    eval_dataset=tokenized_wikiann[\"validation\"],\n    tokenizer=roberta_tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer_lora.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:07:42.861541Z","iopub.execute_input":"2023-08-19T11:07:42.862526Z","iopub.status.idle":"2023-08-19T11:31:53.664891Z","shell.execute_reply.started":"2023-08-19T11:07:42.862491Z","shell.execute_reply":"2023-08-19T11:31:53.663862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background: #dd11d7; font-size: 16px; padding: 10px; border-radius: 1px; color: white; text-align: center; font-weight: bold;\">Ende Übung 3</div>","metadata":{}},{"cell_type":"markdown","source":"## Ergebnis \nUnsere finetuned Models können wir nun am Test-Datensatz ausprobieren. Hier laufen wir jedoch in die gleiche Problematik, die wir auch schon im Pre-Processing hatten: Unser Model liefert uns Klassifizierungen auf Token-Ebene, diese müssen wir zum Anzeigen jedoch zuerst wieder auf Wort-Level zusammenfassen. Hierfür gibt es jedoch eine Pipeline, deren Output wir einfach in das Format der `spacy`-Library umwandeln können, um unsere NER-Ergebnisse darzustellen.\n\n<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Setze die Umwandlung der Token-Level Outputs des Models in Klassifizierungen auf Wort-Ebene selbst um.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\ntoken_classifier = pipeline(\"token-classification\", model=roberta_ner, tokenizer=roberta_tokenizer, aggregation_strategy=\"simple\", device=0)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:32:15.668350Z","iopub.execute_input":"2023-08-19T11:32:15.668783Z","iopub.status.idle":"2023-08-19T11:32:15.962665Z","shell.execute_reply.started":"2023-08-19T11:32:15.668745Z","shell.execute_reply":"2023-08-19T11:32:15.961537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordDetokenizer\n\nprint(wikiann[\"test\"][0][\"tokens\"])\ntest_text = TreebankWordDetokenizer().detokenize(wikiann[\"test\"][0][\"tokens\"])\ntoken_classifier(test_text)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:32:33.778085Z","iopub.execute_input":"2023-08-19T11:32:33.778577Z","iopub.status.idle":"2023-08-19T11:32:33.849101Z","shell.execute_reply.started":"2023-08-19T11:32:33.778538Z","shell.execute_reply":"2023-08-19T11:32:33.847927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom spacy import displacy\n\nner_colors_combined = {\n    \"PER\": \"#FF0000\",\n    \"ORG\": \"#FFA500\",\n    \"LOC\": \"#088F8F\",\n}\n\ndef show_result_with_spacy(sample, pipeline):\n    detokenizer = TreebankWordDetokenizer()\n    \n    text = detokenizer.detokenize(sample[\"tokens\"])\n    result = pipeline(text)\n    entities = [{\"start\": e[\"start\"], \"end\": e[\"end\"] , \"label\": e[\"entity_group\"]} for e in result]\n    \n    displacy.render({\"ents\": entities, \"text\": text}, style=\"ent\", manual=True, options={\"colors\": ner_colors_combined})\n\nshow_result_with_spacy(wikiann[\"test\"][0], token_classifier)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:32:55.863168Z","iopub.execute_input":"2023-08-19T11:32:55.863570Z","iopub.status.idle":"2023-08-19T11:32:55.921492Z","shell.execute_reply.started":"2023-08-19T11:32:55.863537Z","shell.execute_reply":"2023-08-19T11:32:55.920236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interaktive Nutzung\nDie Models mit Test-Daten zu verwenden ist schön und gut, wir können uns aber auch ein Interface basteln, mit der wir sie interaktiv mit eigenen Texten ausprobieren können. Dazu nutzen wir wieder die `gradio`-Bibliothek, die hierfür bereits eine `HighlightedText`-Komponente bereitstellt. \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Teste deine Models mit eigenen Texten. Erkennen sie die Orte, Personen, und Organisationen, die du vermuten würdest? Wo liegen die Schwächen der Models, und woran könnten diese Schwächen liegen?","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n\nexamples = [\n    \"Does Chicago have any stores and does John live here?\",\n    \"The United Nations held a conference in Las Vegas.\",\n    \"Was Falco born in Vienna?\"\n]\n\ndef ner(text):\n    result = token_classifier(text)\n    entities = [{\"start\": e[\"start\"], \"end\": e[\"end\"] , \"entity\": e[\"entity_group\"]} for e in result]\n    return {\"text\": text, \"entities\": entities}    \n\ndemo = gr.Interface(ner,\n             gr.Textbox(placeholder=\"Enter sentence here...\"), \n             gr.HighlightedText(),\n             examples=examples)\n\ndemo.launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T11:33:08.067816Z","iopub.execute_input":"2023-08-19T11:33:08.068230Z","iopub.status.idle":"2023-08-19T11:33:15.055878Z","shell.execute_reply.started":"2023-08-19T11:33:08.068176Z","shell.execute_reply":"2023-08-19T11:33:15.054702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weitere Ressourcen\n\n#### Dokumentation\n- Hugging Face Transformers: https://huggingface.co/docs/transformers/index\n- Hugging Face Datasets: https://huggingface.co/docs/datasets/index\n- Hugging Face PEFT: https://huggingface.co/docs/peft/index\n- Adapter-Transformers (eine Library für die Adapter-Methode für PEFT): https://docs.adapterhub.ml/index.html\n\n#### Theorie\n- Parameter-Efficient Fine-Tuning (PEFT):\n  - Eine Serie an ausgezeichneten Artikeln über die Intuition hinter den diversen Methoden:\n    - https://magazine.sebastianraschka.com/p/finetuning-large-language-models\n    - https://magazine.sebastianraschka.com/p/understanding-parameter-efficient\n    - https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters\n    - https://lightning.ai/pages/community/article/understanding-llama-adapters/\n  - Die ursprünglichen Papers der diversen Methoden:\n    - Soft Prompt-Tuning: https://arxiv.org/abs/2104.08691\n    - Prefix Tuning: https://arxiv.org/abs/2101.00190\n    - Adapters: https://arxiv.org/abs/1902.00751\n    - LoRa: https://arxiv.org/abs/2106.09685\n- Transformer:\n  - Eine weniger technische, dafür intuitive Erklärung der Transformer-Architektur: https://jalammar.github.io/illustrated-transformer/\n  - Eine sehr technische Zusammenfassung der Transformer Architektur: https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/\n  - Das originale Transformer Paper mit Code-Umsetzung: http://nlp.seas.harvard.edu/annotated-transformer/\n","metadata":{}}]}