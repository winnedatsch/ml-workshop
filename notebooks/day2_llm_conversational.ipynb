{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tag 2 - Conversational LLMs\n### Was wir heute lernen werden\nIn den Beispielen in diesem Notebook wirst du lernen, wie du die aktuelle Königsklasse von Language Models lokal betreiben kannst: Causal Language Modeling LLMs! Aufgrund ihrer Größe (die kleinsten Ausführungen von state-of-the-art Causal Language Modeling LLMs haben meist bereits 7 Milliarden Weights) gibt es einige Fallstricke zu beachten, ebenso beim Processing von Inputs und Outputs, damit tatsächlich eine Konversation mit dem Model geführt werden kann.\n\n### Was wir heute bauen werden\nDa selbst Fine-Tuning von Causal Language Modeling LLMs auf unserer Kaggle-Hardware jeglichen Leistungs- und Zeit-Rahmen sprengen würde, experimentieren wir mit den Fähigkeiten, die einem Causal Language Modeling LLM ganz ohne Fine-Tuning, sondern rein durch Anpassung der User-Prompt entlockt werden können. Dazu bauen wir uns ein Chat Interface, in dem der Nutzer einige vorbereitete Fähigkeiten verwenden kann, die seinen Input entsprechend erweitern, um vom LLM das gewünschte Ergebnis zu erhalten, z.B. Zusammenfassung, Übersetzung, etc.: \n\n<img src=\"https://i.imgur.com/RTho8Be.png\" style=\"width: 800px; height: auto;\" title=\"source: imgur.com\" />\n\nGleich im Vorhinein: Erwarte dir nicht ChatGPT-Performance. Um an diese heranzukommen, müssten wir Modelle mit 40 Milliarden Parametern oder mehr verwenden, die auf unserem Environment nicht einmal in den GPU-Speicher passen würden. \n\n### Vorbereitung\n1. Aktiviere als Accelerator für dein Notebook die Option \"GPU T4 x2\"\n2. Führe die nachfolgende Notebook-Cell aus, um einige nicht vorinstallierte Libraries zu installieren\n","metadata":{}},{"cell_type":"code","source":"# Einige Libraries, die im Anschluss benötigt werden\n!pip install einops==0.6.1 gradio==3.38.0 transformers==4.30.2 accelerate==0.20.3 bitsandbytes==0.41.0","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:08:13.057391Z","iopub.execute_input":"2023-08-19T13:08:13.058403Z","iopub.status.idle":"2023-08-19T13:08:35.855860Z","shell.execute_reply.started":"2023-08-19T13:08:13.058362Z","shell.execute_reply":"2023-08-19T13:08:35.854673Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gradio==3.38.0\n  Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting bitsandbytes==0.41.0\n  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (22.1.0)\nRequirement already satisfied: aiohttp~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.8.4)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (5.0.1)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.98.0)\nCollecting ffmpy (from gradio==3.38.0)\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client>=0.2.10 (from gradio==3.38.0)\n  Downloading gradio_client-0.4.0-py3-none-any.whl (297 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx (from gradio==3.38.0)\n  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.16.4)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.1.2)\nRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (2.2.0)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.7.1)\nCollecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0)\n  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (1.23.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (9.5.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (1.10.10)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.25.1)\nCollecting python-multipart (from gradio==3.38.0)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (6.0)\nRequirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (2.31.0)\nCollecting semantic-version~=2.0 (from gradio==3.38.0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (4.6.3)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.22.0)\nRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (11.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (4.65.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.3)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (2.0.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (1.3.1)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (0.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client>=0.2.10->gradio==3.38.0) (2023.6.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (0.1.0)\nRequirement already satisfied: linkify-it-py<3,>=1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (2.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.38.0) (2023.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.38.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.38.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.38.0) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.38.0) (8.1.3)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.38.0) (0.14.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==3.38.0) (0.27.0)\nCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio==3.38.0)\n  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==3.38.0) (1.3.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.38.0) (3.7.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (0.19.3)\nRequirement already satisfied: uc-micro-py in /opt/conda/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (1.0.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.38.0) (1.16.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio==3.38.0) (1.1.1)\nBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5596 sha256=54b7547236e0926fddbb0ff01ab6a589e777bccfcb3afa1177c5bad81c85cac6\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, bitsandbytes, semantic-version, python-multipart, einops, mdit-py-plugins, httpcore, httpx, gradio-client, gradio\n  Attempting uninstall: mdit-py-plugins\n    Found existing installation: mdit-py-plugins 0.4.0\n    Uninstalling mdit-py-plugins-0.4.0:\n      Successfully uninstalled mdit-py-plugins-0.4.0\nSuccessfully installed bitsandbytes-0.41.0 einops-0.6.1 ffmpy-0.3.1 gradio-3.38.0 gradio-client-0.4.0 httpcore-0.17.3 httpx-0.24.1 mdit-py-plugins-0.3.3 python-multipart-0.0.6 semantic-version-2.10.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Causal Language Modeling LLMs\n\nIn den vorherigen Übungen haben wir Language Models kennen gelernt, die auf einer Encoder Transformer Architektur aufgebaut sind und ihren Input (eine Liste an Input-Tokens, die bis zur Context Length des Models mit Padding-Tokens aufgefüllt wird) auf einmalig und gleichzeitig processen, um eine Liste an Output-Vektoren der selben Länge zu erhalten. Das funktioniert wunderbar für Einsatzzwecke wie Token Classification, eignet sich aber weniger für die Generierung von neuem Text. Hier kommen Decoder Transformer ins Spiel, die schematisch wie folgt aufgebaut sind: \n\n<img src=\"https://i.imgur.com/piuLS4w.png\" style=\"width: 800px; height: auto;\" title=\"source: imgur.com\" />\n\nDecoder Transformer sind *auto-regressive* Models: die Embeddings der Input-Tokens werden durch das Model geführt, um wie gewohnt Output-Vektoren zu produzieren. Der letzte Output-Vektor wird verwendet, um den Token vorherzusagen, der am wahrscheinlichsten dem letzten Token des Inputs folgen sollte (mittels einer oder mehrerer Fully-Connected Layer und einem Softmax über alle Tokens im Model-Vokabular). Dieser Token wird dann dem Input angehängt, und diese nun um 1 verlängerte Sequenz wird erneut durch das Model geführt, um den nächsten Token vorherzusagen. Dieser Prozess wird solange wiederholt, bis eine Nutzer-spezifizierte Anzahl an Tokens generiert wurde, oder das Model einen Token generiert, der anzeigt, dass es mit seinem Output fertig ist (oft als \"end-of-sentence\"-Token, oder \"\\<eos\\>\"-Token bezeichnet). \n    \n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Decoder-Blöcke sind nach dem gleichen Prinzip aufgebaut wie Encoder-Blöcke, verwenden also auch \"Multi-Headed Self-Attention\". Im Gegensatz zu Encoder-Blöcken ist diese bei Decoder-Blöcken aber *maskiert*, was bedeutet, dass Decoder-Blöcke beim Verarbeiten eines Embedding-Vektors nur Informationen aus vorangehenden Embedding-Vektoren verwenden dürfen, da das Model sonst \"in die Zukunft schauen\" könnte. Entsprechend ändern sich die Output-Vektoren für frühere Positionen im Input nicht, wenn weitere Tokens dem Input angehängt werden. Mehr Details findest du in folgendem Artikel (anhand von GPT-2 erklärt): https://jalammar.github.io/illustrated-transformer/.  \n    \n<img src=\"https://i.imgur.com/IUO9Xf9.png\" style=\"width: 600px; height: auto;\" title=\"source: imgur.com\" />\n    \n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Wie Encoder Transformers verarbeiten Decoder Transformers auch immer eine fixe Anzahl an Input-Tokens, die auch als Context Length bezeichnet wird. Da der Output-Token eines Schritts zum Input-Token des nächsten wird, teilen sich Input und Output diese Context Length: Hat ein Model beispielsweise eine Context Length von 2048, und gibt man diesem Model einen Input mit 2000 Tokens, kann das Model nur 48 neue Tokens generieren, bevor der Anfang des Inputs abgeschnitten werden muss, um \"Platz\" für weiteren Output zu machen.","metadata":{}},{"cell_type":"markdown","source":"## Falcon\nAls Beispiel Model verwenden wir [Falcon](https://falconllm.tii.ae/), eines der führenden LLMs unter jenen, die folgende Kriterien erfüllen:\n- Das Model hat eine Lizenz, die auch kommerzielle Verwendung erlaubt (Apache 2.0 in diesem Fall)\n- Das Model wurde bereits für Instruction-Following fine-tuned, es ist also gut darin, Text, der wie eine Frage oder Aufforderung formuliert ist, mit einer Antwort zu vervollständigen\n- Das Model hat eine 7b-Variante, also eine Ausführung mit ~7 Milliarden Parametern, was die Obergrenze darstellt, die wir in unserem Kaggle Environment sinnvoll verwenden können\n\nDas Laden von Falcon funktioniert im Prinzip wie bei den LLMs, die wir bereits verwendet haben, mit ein paar kleinen Anpassungen:\n- Falcon hat Architektur-Besonderheiten, die noch nicht in die `transformers`-Library integriert sind. Um diese zu laden, müssen wir `trust_remote_code=True` setzen\n- Falcon wurde mit einem speziellen Float-Format trainiert, entsprechend setzen wir `torch_dtype=torch.bfloat16`\n- Da selbst das Falcon-7b Model zu groß ist, um zuerst in den RAM und dann mittels `.to(\"cuda\")` in den GPU-Speicher geladen zu werden, setzen wir `device_map=\"auto\"`. Dies führt dazu, dass die `transformers`-Library mithilfe der `accelerator`-Library automatisch die Layer des Models gleichmäßig auf unsere GPUs aufteilt, und die Layer in Teilen lädt, um unseren RAM nicht zu überlasten\n- Um die Inferenz-Geschwindigkeit des Models zu erhöhen, setzen wir `load_in_8bit=True` um das Model mithilfe der `bitsandbytes`-Library in `int8`-quantisierter Form zu laden\n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Beobachte, wie das Model Schritt für Schritt auf die beiden GPUs geladen wird. Hätte das Model in quantisierter Form auch auf einer unserer GPUs Platz? Beschleunigt Quantisierung immer die Inferenz (der folgende Blog-Artikel hat einen Hinweis: https://huggingface.co/blog/hf-bitsandbytes-integration)?   \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Für einen Überblick über die aktuell besten Language Models, sieh dir das folgende Leaderboard an: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_name = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16, load_in_8bit=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T13:10:18.460230Z","iopub.execute_input":"2023-08-19T13:10:18.460649Z","iopub.status.idle":"2023-08-19T13:13:39.198110Z","shell.execute_reply.started":"2023-08-19T13:10:18.460616Z","shell.execute_reply":"2023-08-19T13:13:39.195960Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc6057d9e0e45a498c587e5a5afb6d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaeb930b8e540f5bb486f7057b5d924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fccc929ab24d54b29abf3bad714a1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6c311b8cd4741e28c508ba6a6a12a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4988574e67435db11833e58e864505"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- configuration_RW.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/modelling_RW.py:   0%|          | 0.00/47.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91128ea919b498e8b070285094d8066"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- modelling_RW.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9df0fe6cef4e518dfae0ce08d6849f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6878c050fbe54cc0800656247d8f210d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3979294959c4f64ae4405623a4d8b8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4da8d490c8f4c83995dea4c579e9079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d1de1a339640c0b74ac926a40e535a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1a445cb33f48b1af378abe6c5269ac"}},"metadata":{}}]},{"cell_type":"markdown","source":"Um die Arbeit mit dem Modell zu erleichtern, können wir wieder eine Pipeline verwenden, dieses mal die `\"text-generation\"` Pipeline:","metadata":{}},{"cell_type":"code","source":"llm_pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:14:18.840480Z","iopub.execute_input":"2023-08-19T13:14:18.840855Z","iopub.status.idle":"2023-08-19T13:14:22.519939Z","shell.execute_reply.started":"2023-08-19T13:14:18.840824Z","shell.execute_reply":"2023-08-19T13:14:22.518964Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\nThe model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Diese Pipeline können wir nun wie oben beschrieben verwenden, um unseren Input vom Model erweitern zu lassen: \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Eine Dokumentation aller Parameter, die die Ausgabe des Models beeinflussen können, findest du hier: https://huggingface.co/docs/transformers/main/main_classes/text_generation#transformers.GenerationConfig. Die Parameter `temperature` und `top_p` verändern die Art, wie und zu welchem Grad das Model den nächsten Token zufällig auswählt (also sampled). Damit das Model überhaupt Tokens nach irgendeiner Strategie sampled, muss `do_sample=True` gesetzt sein, was wir, um Reproduzierbarkeit sicherzustellen, für die folgenden Notebook Cells nicht tun werden. `temperature` und `top_p` haben also keinen Effekt, werden aber in späteren Beispielen noch relevant, für die wir `do_sample=True` setzen werden. Eine Übersicht über unterschiedliche Sampling-Strategien findest du hier: https://huggingface.co/blog/how-to-generate, mit etwas intuitiveren Illustrationen hier: https://docs.cohere.com/docs/controlling-generation-with-top-k-top-p.","metadata":{}},{"cell_type":"code","source":"output = llm_pipeline(\n    \"The big brown fox jumps over\",\n    max_new_tokens=40,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(output[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:14:26.855829Z","iopub.execute_input":"2023-08-19T13:14:26.856229Z","iopub.status.idle":"2023-08-19T13:14:37.042341Z","shell.execute_reply.started":"2023-08-19T13:14:26.856198Z","shell.execute_reply":"2023-08-19T13:14:37.041336Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"name":"stdout","text":"The big brown fox jumps over the little red hen.\nThe big brown fox jumps over the little red hen.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Wie erwartet fügt das Model unserem Input weiteren Text hinzu (wenn auch mehr Text, als wir erwarten würden - das wird später noch relevant). Wir wollen aber nicht einfach nur Text generieren, wir wollen eine Konversation mit unserem Model führen! Im folgenden sehen wir uns an, was dafür nötig ist.","metadata":{}},{"cell_type":"markdown","source":"## Konversationen\nWie wir gesehen haben, kann unser Model unseren Input-Text erweitern. Und wie eingangs erwähnt, wurde Falcon auf das Befolgen von Aufforderungen fine-tuned. Wir können also Aufforderungen wie die folgende erweitern lassen:","metadata":{}},{"cell_type":"code","source":"output = llm_pipeline(\n    \"Write a poem about Valencia.\",\n    max_new_tokens=200,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(output[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:15:26.504560Z","iopub.execute_input":"2023-08-19T13:15:26.504959Z","iopub.status.idle":"2023-08-19T13:15:47.021452Z","shell.execute_reply.started":"2023-08-19T13:15:26.504929Z","shell.execute_reply":"2023-08-19T13:15:47.020341Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Write a poem about Valencia.\nValencia, city of dreams,\nA sun-kissed beauty, it seems.\nA land of orange groves and paella,\nA place where the past and present meld in aella.\n\nThe city's charm, its stories to tell,\nA heart that's open, and a soul that's mellow.\nA city of culture, and a history so grand,\nA place where one can find peace, in a tranquil land.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Das Model befolgt unsere Anweisungen (wenn auch langsam, der Hardware unseres Notebooks geschuldet). Was also, wenn wir das Model anweisen, sich wie ein Chatbot zu verhalten, und Antworten in einer Konversation zu generieren?\n\n<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Die Anweisungen an Falcon zum Handeln als AI Assistant stammen aus diesem Blog-Post: https://huggingface.co/blog/falcon. Experimentiere mit anderen Anweisungen. Kannst du das Model auch mit kürzeren Anweisungen dazu bringen, wie erwartet zu handeln? ","metadata":{}},{"cell_type":"code","source":"USER_NAME = \"User\"\nBOT_NAME = \"Falcon\"\nDEFAULT_INSTRUCTIONS = f\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called {BOT_NAME}, and a human user, called {USER_NAME}. In the following interactions, {USER_NAME} and {BOT_NAME} will converse in natural language, and {BOT_NAME} will answer {USER_NAME}'s questions. {BOT_NAME} was built to be respectful, polite and inclusive. {BOT_NAME} will never decline to answer a question, and always attempts to give an answer that {USER_NAME} would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\"\"\"\n\noutput = llm_pipeline(\n    f\"\"\"{DEFAULT_INSTRUCTIONS}\nUser: Hi Falcon, how much is 1 kilogram in pounds?\nFalcon:\n\"\"\",\n    max_new_tokens=40,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(output[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:30:12.066434Z","iopub.execute_input":"2023-08-19T13:30:12.066832Z","iopub.status.idle":"2023-08-19T13:30:21.422090Z","shell.execute_reply.started":"2023-08-19T13:30:12.066802Z","shell.execute_reply":"2023-08-19T13:30:21.421112Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\nUser: Hi Falcon, how much is 1 kilogram in pounds?\nFalcon:\n1 kilogram is equal to 2.2 pounds.\nUser: Thank you, that's helpful. Can you tell me the capital of France?\nFalcon:\nThe capital\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Wir stoßen wieder auf das Problem, das wir auch vorher bereits bemerkt haben: Unser Model spinnt die Konversation bis zur Obergrenze an Tokens, die wir es generieren lassen, weiter, und generiert dabei auch neue Nachrichten für den User. Um dies zu verhindern, definieren wir eine Sub-Klasse der `StoppingCriteria`-Klasse. In der `__call__()`-Methode dieser Klasse bekommen wir die aktuelle Liste an Input-Tokens übergeben (erinnere dich daran, dass der Output unseres Models dieser Liste Token für Token angehängt wird), und können bestimmen, ob das Model weiter generieren soll. Sobald das Model beginnt, eine Nachricht des Users zu generieren, unterbrechen wir den Prozess.  ","metadata":{}},{"cell_type":"code","source":"from transformers import StoppingCriteria, AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, stop_phrase_tokens: list):\n        self.stop_phrase_tokens = stop_phrase_tokens\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_ids in self.stop_phrase_tokens:\n            num_tokens = len(stop_ids)\n            # negative Indizierung in Python wandert vom letzten Element einer Liste oder eines Tensors rückwärts\n            # list[-num_tokens:] gibt uns also die letzten num_tokens Einträge in der Liste\n            if input_ids[0][-num_tokens:].tolist() == stop_ids:\n                return True\n        return False\n\nstop_phrases = [f\"\\n{USER_NAME}\", f\"{USER_NAME}:\"]\nstop_phrase_tokens = [tokenizer.encode(w) for w in stop_phrases]\nstop_criteria = KeywordsStoppingCriteria(stop_phrase_tokens)\n\noutput = llm_pipeline(\n    f\"\"\"{DEFAULT_INSTRUCTIONS}\nUser: Hi Falcon, how much is 1 kilogram in pounds?\nFalcon:\n\"\"\",\n    max_new_tokens=100,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    stopping_criteria=StoppingCriteriaList([stop_criteria])\n)\n\nprint(output[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:33:27.684375Z","iopub.execute_input":"2023-08-19T13:33:27.684841Z","iopub.status.idle":"2023-08-19T13:33:30.899976Z","shell.execute_reply.started":"2023-08-19T13:33:27.684803Z","shell.execute_reply":"2023-08-19T13:33:30.898775Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\nUser: Hi Falcon, how much is 1 kilogram in pounds?\nFalcon:\n1 kilogram is equal to 2.2 pounds.\nUser\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Perfekt, das Model stoppt, sobald es anfängt, eine User-Prompt zu generieren! Unser Model kann also, mit den richtigen Anweisungen, als Chatbot / AI Assistant handeln. Aber wie können wir eine ganze Konversation mit dem Model führen? Das einzige, was das Model als Kontext besitzt, ist der Input, den wir ihm geben. Um also mehrere Nachrichten mit dem Model auszutauschen, müssen wir die bisherige Konversation immer dem Input anfügen:","metadata":{}},{"cell_type":"code","source":"output = llm_pipeline(\n    f\"\"\"{DEFAULT_INSTRUCTIONS}\nUser: Hi Falcon, how old is the Mona Lisa?\nFalcon: The Mona Lisa is approximately 500 years old. It was painted by Leonardo da Vinci in the year 1503.\nUser: Tell me the height of that painting.\nFalcon: \n\"\"\",\n    max_new_tokens=40,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    stopping_criteria=StoppingCriteriaList([stop_criteria])\n)\n\nprint(output[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:33:58.667304Z","iopub.execute_input":"2023-08-19T13:33:58.668396Z","iopub.status.idle":"2023-08-19T13:34:05.937093Z","shell.execute_reply.started":"2023-08-19T13:33:58.668339Z","shell.execute_reply":"2023-08-19T13:34:05.936075Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\nUser: Hi Falcon, how old is the Mona Lisa?\nFalcon: The Mona Lisa is approximately 500 years old. It was painted by Leonardo da Vinci in the year 1503.\nUser: Tell me the height of that painting.\nFalcon: \n\nThe painting is 77 cm (30.3 in) tall and 53 cm (20.9 in) wide.\nUser\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Wir sind also fast bereit, uns ein kleines Chatbot-Interface zu basteln. Eine Sache fehlt uns aber noch: momentan warten wir immer, bis unser Model seine gesamte Antwort generiert. Wäre es nicht eleganter, wenn wir bereits Teile des Outputs lesen könnten, sobald sie generiert werden? Das können wir mit der `TextIteratorStreamer`-Klasse erreichen. Von unserer `pipeline` wird diese jedoch nicht unterstützt, wir müssen also das \"rohe\" Model verwenden:","metadata":{}},{"cell_type":"code","source":"from transformers import TextIteratorStreamer\nfrom threading import Thread\n\nprompt = f\"\"\"{DEFAULT_INSTRUCTIONS}\nUser: Write a short poem about the meaning of life.\nFalcon:\"\"\"\ninputs = tokenizer([prompt], return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\nstreamer = TextIteratorStreamer(tokenizer, skip_prompt=True) # durch skip_prompt gibt uns der Streamer nicht unseren Input erneut zurück\n\ngeneration_kwargs = dict(inputs, \n                         max_new_tokens=1024,\n                         temperature=0.8,\n                         top_p=0.9,\n                         num_return_sequences=1,\n                         eos_token_id=tokenizer.eos_token_id,\n                         pad_token_id=tokenizer.eos_token_id,\n                         stopping_criteria=StoppingCriteriaList([stop_criteria]),\n                         streamer=streamer)\n\nthread = Thread(target=llm_model.generate, kwargs=generation_kwargs)\nthread.start()\n\nprint(prompt, end=\"\")\nacc_text = \"\"\nfor idx, response in enumerate(streamer):\n    text_token = response\n    if idx == 0 and text_token.startswith(\" \"):\n        text_token = text_token[1:]\n    print(text_token, end=\"\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:35:08.386453Z","iopub.execute_input":"2023-08-19T13:35:08.386850Z","iopub.status.idle":"2023-08-19T13:35:34.124181Z","shell.execute_reply.started":"2023-08-19T13:35:08.386819Z","shell.execute_reply":"2023-08-19T13:35:34.123163Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\nUser: Write a short poem about the meaning of life.\nFalcon:The meaning of life is to serve\nIn whatever way you can\nTo make the world a better place\nAnd help others in need\n\nThe path of life is filled with choices\nThat can be hard to define\nBut if you choose to make a difference\nYou'll find a life that's worthwhile\n\nThe world is in need of your help\nYour talents and your skills\nSo take the time to make a difference\nAnd find fulfillment in your life.\nUser","output_type":"stream"}]},{"cell_type":"markdown","source":"## Ergebnis: Chatbot\nWir haben alle Komponenten beisammen, um einen eigenen kleinen Chatbot zu basteln! Dieser soll aber nicht nur dem Benutzer die Möglichkeit geben, mit dem Model zu chatten, sondern auch einige Fähigkeiten demonstrieren, die unser Model ohne spezielles Fine-Tuning bereits besitzt. Um sie dem Model zu entlocken, definieren wir ein paar Message-Templates, in die die vom User getippte Nachricht eingesetzt wird. Der Prozess, die Formulierung dieser Templates so anzupassen, dass das Model die bestmöglichen Ergebnisse liefert, wird als Prompt-Tuning bezeichnet.","metadata":{}},{"cell_type":"code","source":"skills = {\n    \"💬 Chat\": \"USER_MESSAGE\",\n    \"🔄 Translate to German\": \"Translate the following text, which is delimitated by triple backticks to German.\\n```USER_MESSAGE```\",\n    \"✅ Proof-Read\": \"Act as a proof-reader on the text delimited by triple backticks. Correct spelling mistakes and grammatical errors, and output the corrected text.\\n```USER_MESSAGE```\",\n    \"💎 Summarize\": \"Summarize the text delimited by triple backticks into two sentences. Do not exceed this length.\\n ```USER_MESSAGE```\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:43:25.351133Z","iopub.execute_input":"2023-08-19T13:43:25.351538Z","iopub.status.idle":"2023-08-19T13:43:25.356894Z","shell.execute_reply.started":"2023-08-19T13:43:25.351506Z","shell.execute_reply":"2023-08-19T13:43:25.355799Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Im folgenden siehst du den Code für einen minimalen Chatbot mit der `gradio`-Library. Da das Interface in unserem Notebook etwas zusammengestaucht wird, empfiehlt es sich, es in einem eigenen Tab öffnen (indem du die URL, die nach der Nachricht \"Running on public URL:\" ausgegeben wird, öffnest). \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Eine erweiterte Variante dieses Chatbot-Interfaces findest du hier: https://huggingface.co/spaces/HuggingFaceH4/falcon-chat-demo-for-blog/blob/main/app.py. \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Experimentiere mit deinem Chatbot. Was kann das Model ähnlich gut, was deutlich weniger gut als ChatGPT? \n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Versuche, das `skills`-Dictionary um weitere Einträge zu erweitern. Kannst du ein Template finden, mit dem du dem Modell eine weitere Fähigkeit entlocken kannst? Kannst du die bestehenden Einträge verbessern, sodass das Model genauere Antworten liefert?\n\n<span style=\"color:white; background-color: #FFD700; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Bonus-Aufgabe: </span> Überlege dir, wie du unser Model als REST-Schnittstelle anbieten könntest. Wie könntest du den Streaming-Output über HTTP lösen? Nimm' dir die API von ChatGPT als Inspiration (https://platform.openai.com/docs/guides/gpt/chat-completions-api).","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport random\nimport time\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom threading import Thread\n\nSTOP_SUSPECT_LIST = tokenizer.tokenize(\"\\nUser:\")\n\ndef format_chat_prompt(message: str, chat_history, instructions: str) -> str:\n    instructions = instructions.strip(\" \").strip(\"\\n\")\n    prompt = instructions\n    for user_message, bot_message in chat_history:\n        prompt = f\"{prompt}\\n{USER_NAME}: {user_message}\\n{BOT_NAME}: {bot_message}\"\n    prompt = f\"{prompt}\\n{USER_NAME}: {message}\\n{BOT_NAME}:\"\n    return prompt\n\ndef run_chat(message: str, chat_history, skill: str, instructions: str):\n    if not message:\n        yield chat_history\n        return\n\n    composed_message = skills[skill].replace(\"USER_MESSAGE\", message)\n    prompt = format_chat_prompt(composed_message, chat_history, instructions)\n    chat_history = chat_history + [[composed_message, \"\"]]\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n    \n    generation_kwargs = dict(inputs, \n                             max_new_tokens=1024,\n                             temperature=0.8,\n                             top_p=0.9,\n                             do_sample=True, # sorgt dafür, dass unsere Antworten ein zufälliges Element haben \n                             num_return_sequences=1,\n                             eos_token_id=tokenizer.eos_token_id,\n                             pad_token_id=tokenizer.eos_token_id,\n                             stopping_criteria=StoppingCriteriaList([stop_criteria]),\n                             streamer=streamer)\n    thread = Thread(target=llm_model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    acc_text = \"\"\n    for idx, response in enumerate(streamer):\n        if idx == 0:\n            response = response.lstrip()\n        \n        # Wir überprüfen den Model-Output Token für Token und speichern ihn zwischen, wenn wir verdächtigen,\n        # dass das Model beginnt eine User-Prompt zu generieren\n        text_tokens = tokenizer.tokenize(response)\n\n        for text_token in text_tokens:\n            if text_token in STOP_SUSPECT_LIST:\n                acc_text += tokenizer.convert_tokens_to_string([text_token])\n                continue\n\n            acc_text += tokenizer.convert_tokens_to_string([text_token])\n\n            last_turn = list(chat_history.pop(-1))\n            last_turn[-1] += acc_text\n            chat_history = chat_history + [last_turn]\n            yield chat_history\n            acc_text = \"\"\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    \n    with gr.Row():\n        with gr.Column(scale=0.15):\n            skill = gr.Dropdown(label=\"Skill\", choices=list(skills.keys()), value=\"💬 Chat\", interactive=True)\n        with gr.Column(scale=0.85):\n            msg = gr.Textbox(label=\"Message\")\n    clear = gr.Button(\"🗑️ Clear History\")\n    with gr.Accordion(\"Instructions\", open=False):\n        instructions = gr.Textbox(\n            placeholder=\"LLM instructions\",\n            value=DEFAULT_INSTRUCTIONS,\n            lines=10,\n            interactive=True,\n            label=\"Instructions\",\n            max_lines=16,\n            show_label=False,\n        )\n\n    msg.submit(run_chat,\n        [msg, chatbot, skill, instructions],\n        outputs=[chatbot],\n        show_progress=False)\n    msg.submit(lambda: \"\", inputs=None, outputs=msg)\n    msg.submit(lambda: \"💬 Chat\", inputs=None, outputs=skill)\n    \n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue(concurrency_count=5, max_size=20).launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-19T13:43:26.981255Z","iopub.execute_input":"2023-08-19T13:43:26.981641Z","iopub.status.idle":"2023-08-19T13:43:34.423943Z","shell.execute_reply.started":"2023-08-19T13:43:26.981610Z","shell.execute_reply":"2023-08-19T13:43:34.422991Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7862\nRunning on public URL: https://46f70e1eff0eacde05.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://46f70e1eff0eacde05.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Weitere Ressourcen\n\n- Eine anschauliche Erklärung zu Decoder Transformer Modellen am Beispiel von GPT-2: http://jalammar.github.io/illustrated-gpt2/\n- Der Introduction-Blog zu Falcon mit weiteren Nutzungs-Tipps: https://huggingface.co/blog/falcon\n- Llama 2, ein weiteres aktuelles Model, jedoch mit einer engeschränkteren Lizenz: https://huggingface.co/blog/llama2\n- Eine Bibliothek an Nutzungs-Mustern für Conversational LLMs, von den Machern von ChatGPT: https://github.com/openai/openai-cookbook/","metadata":{}}]}