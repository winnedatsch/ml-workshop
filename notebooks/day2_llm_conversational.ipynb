{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tag 2 - Conversational LLMs\n### Was wir heute lernen werden\nIn den Beispielen in diesem Notebook wirst du lernen, wie du die aktuelle KÃ¶nigsklasse von Language Models lokal betreiben kannst: Causal Language Modeling LLMs! Aufgrund ihrer GrÃ¶ÃŸe (die kleinsten AusfÃ¼hrungen von state-of-the-art Causal Language Modeling LLMs haben meist bereits 7 Milliarden Weights) gibt es einige Fallstricke zu beachten, ebenso beim Processing von Inputs und Outputs, damit tatsÃ¤chlich eine Konversation mit dem Model gefÃ¼hrt werden kann.\n\n### Was wir heute bauen werden\nDa selbst Fine-Tuning von Causal Language Modeling LLMs auf unserer Kaggle-Hardware jeglichen Leistungs- und Zeit-Rahmen sprengen wÃ¼rde, experimentieren wir mit den FÃ¤higkeiten, die einem Causal Language Modeling LLM ganz ohne Fine-Tuning, sondern rein durch Anpassung der User-Prompt entlockt werden kÃ¶nnen. Dazu bauen wir uns ein Chat Interface, in dem der Nutzer einige vorbereitete FÃ¤higkeiten verwenden kann, die seinen Input entsprechend erweitern, um vom LLM das gewÃ¼nschte Ergebnis zu erhalten, z.B. Zusammenfassung, Ãœbersetzung, etc.: \n\n### Vorbereitung\n1. Aktiviere als Accelerator fÃ¼r dein Notebook die Option \"GPU T4 x2\"\n2. FÃ¼hre die nachfolgende Notebook-Cell aus, um einige nicht vorinstallierte Libraries zu installieren\n","metadata":{}},{"cell_type":"code","source":"# Einige Libraries, die im Anschluss benÃ¶tigt werden\n!pip install einops==0.6.1 gradio==3.38.0 transformers==4.30.2 accelerate==0.20.3 bitsandbytes==0.41.0","metadata":{"execution":{"iopub.status.busy":"2023-07-24T16:16:29.666747Z","iopub.execute_input":"2023-07-24T16:16:29.667048Z","iopub.status.idle":"2023-07-24T16:16:54.073949Z","shell.execute_reply.started":"2023-07-24T16:16:29.667020Z","shell.execute_reply":"2023-07-24T16:16:54.072788Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gradio==3.38.0\n  Downloading gradio-3.38.0-py3-none-any.whl (19.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting bitsandbytes==0.41.0\n  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (22.1.0)\nRequirement already satisfied: aiohttp~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.8.4)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (5.0.1)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.98.0)\nCollecting ffmpy (from gradio==3.38.0)\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client>=0.2.10 (from gradio==3.38.0)\n  Downloading gradio_client-0.2.10-py3-none-any.whl (288 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httpx (from gradio==3.38.0)\n  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.16.4)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.1.2)\nRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (2.2.0)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.7.1)\nCollecting mdit-py-plugins<=0.3.3 (from gradio==3.38.0)\n  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (1.23.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (3.9.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (9.5.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (1.10.10)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.25.1)\nCollecting python-multipart (from gradio==3.38.0)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (6.0)\nRequirement already satisfied: requests~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (2.31.0)\nCollecting semantic-version~=2.0 (from gradio==3.38.0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (4.6.3)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (0.22.0)\nRequirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.38.0) (11.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (3.12.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (4.65.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.3)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (2.0.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp~=3.0->gradio==3.38.0) (1.3.1)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (4.17.3)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.38.0) (0.12.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client>=0.2.10->gradio==3.38.0) (2023.6.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (0.1.0)\nRequirement already satisfied: linkify-it-py<3,>=1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (2.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.38.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.38.0) (2023.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.38.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.38.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.0->gradio==3.38.0) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.38.0) (8.1.3)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.38.0) (0.14.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==3.38.0) (0.27.0)\nCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio==3.38.0)\n  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==3.38.0) (1.3.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.38.0) (3.7.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.38.0) (0.19.3)\nRequirement already satisfied: uc-micro-py in /opt/conda/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.38.0) (1.0.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.38.0) (1.16.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio==3.38.0) (1.1.1)\nBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5596 sha256=bb541e1bab86ade45f556438506f1a7ceac29c9e58c32161c202a1ae3eb534d4\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, bitsandbytes, semantic-version, python-multipart, einops, mdit-py-plugins, httpcore, httpx, gradio-client, gradio\n  Attempting uninstall: mdit-py-plugins\n    Found existing installation: mdit-py-plugins 0.4.0\n    Uninstalling mdit-py-plugins-0.4.0:\n      Successfully uninstalled mdit-py-plugins-0.4.0\nSuccessfully installed bitsandbytes-0.41.0 einops-0.6.1 ffmpy-0.3.1 gradio-3.38.0 gradio-client-0.2.10 httpcore-0.17.3 httpx-0.24.1 mdit-py-plugins-0.3.3 python-multipart-0.0.6 semantic-version-2.10.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Causal Language Modeling LLMs\n\nIn den vorherigen Ãœbungen haben wir Language Models kennen gelernt, die auf einer Encoder Transformer Architektur aufgebaut sind und ihren Input (eine Liste an Input-Tokens, die bis zur Context Length des Models mit Padding-Tokens aufgefÃ¼llt wird) auf einmalig und gleichzeitig processen, um eine Liste an Output-Vektoren der selben LÃ¤nge zu erhalten. Das funktioniert wunderbar fÃ¼r Einsatzzwecke wie Token Classification, eignet sich aber weniger fÃ¼r die Generierung von neuem Text. Hier kommen Decoder Transformer ins Spiel, die schematisch wie folgt aufgebaut sind: \n\n<img src=\"https://i.imgur.com/piuLS4w.png\" style=\"width: 800px; height: auto;\" title=\"source: imgur.com\" />\n\nDecoder Transformer sind *auto-regressive* Models: die Embeddings der Input-Tokens werden durch das Model gefÃ¼hrt, um wie gewohnt Output-Vektoren zu produzieren. Der letzte Output-Vektor wird verwendet, um den Token vorherzusagen, der am wahrscheinlichsten dem letzten Token des Inputs folgen sollte (mittels einer oder mehrerer Fully-Connected Layer und einem Softmax Ã¼ber alle Tokens im Model-Vokabular). Dieser Token wird dann dem Input angehÃ¤ngt, und diese nun um 1 verlÃ¤ngerte Sequenz wird erneut durch das Model gefÃ¼hrt, um den nÃ¤chsten Token vorherzusagen. Dieser Prozess wird solange wiederholt, bis eine Nutzer-spezifizierte Anzahl an Tokens generiert wurde, oder das Model einen Token generiert, der anzeigt, dass es mit seinem Output fertig ist (oft als \"end-of-sentence\"-Token, oder \"<eos>\"-Token bezeichnet). \n    \n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Decoder-BlÃ¶cke sind nach dem gleichen Prinzip aufgebaut wie Encoder-BlÃ¶cke, verwenden also auch \"Multi-Headed Self-Attention\". Im Gegensatz zu Encoder-BlÃ¶cken ist diese bei Decoder-BlÃ¶cken aber *maskiert*, was bedeutet, dass Decoder-BlÃ¶cke beim Verarbeiten eines Embedding-Vektors nur Informationen aus vorangehenden Embedding-Vektoren verwenden dÃ¼rfen, da das Model sonst \"in die Zukunft schauen\" kÃ¶nnte. Entsprechend Ã¤ndern sich die Output-Vektoren fÃ¼r frÃ¼here Positionen im Input nicht, wenn weitere Tokens dem Input angehÃ¤ngt werden. Mehr Details findest du in folgendem Artikel (anhand von GPT-2 erklÃ¤rt): https://jalammar.github.io/illustrated-transformer/.  \n    \n<img src=\"https://i.imgur.com/IUO9Xf9.png\" style=\"width: 600px; height: auto;\" title=\"source: imgur.com\" />\n    \n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> Wie Encoder Transformers verarbeiten Decoder Transformers auch immer eine fixe Anzahl an Input-Vektoren, die auch als Context Length bezeichnet wird. Da der Output-Token eines Schritts zum Input-Token des nÃ¤chsten wird, teilen sich Input und Output diese Context Lenght: Hat ein Model beispielsweise eine Context Length von 2048, und gibt man diesem Model einen Input mit 2000 Tokens, kann das Model nur 48 neue Tokens generieren, bevor der Anfang des Inputs abgeschnitten werden muss, um \"Platz\" fÃ¼r weiteren Output zu machen.","metadata":{}},{"cell_type":"markdown","source":"## Falcon\nAls Beispiel Model verwenden wir [Falcon](https://falconllm.tii.ae/), eines der fÃ¼hrenden LLMs unter jenen, die folgende Kriterien erfÃ¼llen:\n- Das Model hat eine Lizenz, die auch kommerzielle Verwendung erlaubt (Apache 2.0 in diesem Fall)\n- Das Model wurde bereits fÃ¼r Instruction-Following fine-tuned, es ist also gut darin, Text mit einer Antwort zu vervollstÃ¤ndigen, der wie eine Frage oder Aufforderung formuliert ist\n- Das Model hat eine 7b-Variante, also eine AusfÃ¼hrung mit ~7 Milliarden Parametern, was die Obergrenze darstellt, die wir in unserem Kaggle Environment sinnvoll verwenden kÃ¶nnen\n\nDas Laden von Falcon funktioniert im Prinzip wie bei den LLMs, die wir bereits verwendet haben, mit ein paar kleinen Anpassungen:\n- Falcon hat Architektur-Besonderheiten, die noch nicht in die `transformers`-Library integriert sind. Um diese zu laden, mÃ¼ssen wir `trust_remote_code=True` setzen\n- Falcon wurde mit einem speziellen Float-Format trainiert, entsprechend setzen wir `torch_dtype=torch.bfloat16`\n- Da selbst das Falcon-7b Model zu groÃŸ ist, um zuerst in den RAM und dann mittels `.to(\"cuda\")` in den GPU-Speicher geladen zu werden, setzen wir `device_map=\"auto\"`. Dies fÃ¼hrt dazu, dass die `transformers`-Library mithilfe der `accelerator`-Library automatisch die Layer des Models gleichmÃ¤ÃŸig auf unsere GPUs aufteilt, und die Layer in Teilen lÃ¤dt, um unseren RAM nicht zu Ã¼berlasten\n- Um die Inferenz-Geschwindigkeit des Models zu erhÃ¶hen, setzen wir `load_in_8bit=True` um das Model mithilfe der `bitsandbytes`-Library in `int8`-quantisierter Form zu laden\n\n<span style=\"color:white; background-color: red; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Aufgabe: </span> Beobachte, wie das Model Schritt fÃ¼r Schritt auf die beiden GPUs geladen wird. HÃ¤tte das Model in quantisierter Form auch auf einer unserer GPUs Platz? Beschleunigt Quantisierung immer die Inferenz (der folgende Blog-Artikel hat einen Hinweis: https://huggingface.co/blog/hf-bitsandbytes-integration)?   \n\n<span style=\"color:white; background-color: blue; padding: 3px 6px; border-radius: 2px; margin-right: 5px;\">Info: </span> FÃ¼r einen Ãœberblick Ã¼ber die aktuell besten Language Models, sieh dir das folgende Leaderboard an: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel_name = \"tiiuae/falcon-7b-instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nllm_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16, load_in_8bit=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-24T15:42:07.208064Z","iopub.execute_input":"2023-07-24T15:42:07.208985Z","iopub.status.idle":"2023-07-24T15:45:44.330097Z","shell.execute_reply.started":"2023-07-24T15:42:07.208935Z","shell.execute_reply":"2023-07-24T15:45:44.329215Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f087bdd2e74f629d0845346c093ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5713a95dd1e3408fa5cfa9becef14b5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad393805513a478b8a4a21e96e59da6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/667 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaa9fd01af374af5afc542d395b7c0d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00bdd595d1d04b2584a7e87bd0182992"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- configuration_RW.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)main/modelling_RW.py:   0%|          | 0.00/47.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2472bba2fa4146ac082f0d0f1e9fb7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b-instruct:\n- modelling_RW.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc340b2c4dd34fbeb88cb045e0b8d3f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb697fba20e34236acf168db63f0c88f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9716466054c34b4bb3ec2b66d06b795d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)l-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6b381209c34c4dbc9e8da099f85bd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1732c9583a04486a8e6b5606dab19f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (â€¦)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d3d057e29cd47ca9c02294800cbf938"}},"metadata":{}}]},{"cell_type":"markdown","source":"Um die Arbeit mit dem Modell zu erleichtern, kÃ¶nnen wir wieder eine Pipeline verwenden, dieses mal die `\"text-generation\"` Pipeline:","metadata":{}},{"cell_type":"code","source":"llm_pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=llm_model,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T15:45:58.174224Z","iopub.execute_input":"2023-07-24T15:45:58.175190Z","iopub.status.idle":"2023-07-24T15:46:01.562748Z","shell.execute_reply.started":"2023-07-24T15:45:58.175122Z","shell.execute_reply":"2023-07-24T15:46:01.561600Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\npip install xformers.\nThe model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Diese Pipeline kÃ¶nnen wir nun wie oben beschrieben verwenden, um unseren Input vom Model erweitern zu lassen: ","metadata":{}},{"cell_type":"code","source":"llm_pipeline(\n    \"The big brown fox jumps over\",\n    max_new_tokens=40,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T16:02:16.130220Z","iopub.execute_input":"2023-07-24T16:02:16.130597Z","iopub.status.idle":"2023-07-24T16:02:22.335068Z","shell.execute_reply.started":"2023-07-24T16:02:16.130564Z","shell.execute_reply":"2023-07-24T16:02:22.333611Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': 'The big brown fox jumps over the little red hen.\\nThe big brown fox jumps over the little red hen.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Wie erwartet fÃ¼gt das Model unserem Input weiteren Text hinzu (wenn auch mehr Text, als wir erwarten wÃ¼rden - das wird spÃ¤ter noch relevant). Wir wollen aber nicht einfach nur Text generieren, wir wollen eine Konversation mit unserem Model fÃ¼hren! Im folgenden sehen wir uns an, was dafÃ¼r nÃ¶tig ist.","metadata":{}},{"cell_type":"markdown","source":"## Konversationen\nWie wir gesehen haben, kann unser Model unseren Input Text erweitern. Und wie eingangs erwÃ¤hnt, wurde Falcon auf das Befolgen von Aufforderungen fine-tuned. Wir kÃ¶nnen also Aufforderungen wie die folgende erweitern lassen:","metadata":{}},{"cell_type":"code","source":"output = llm_pipeline(\n    \"Write a poem about Valencia.\",\n    max_new_tokens=200,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(output[0][\"generated_text\"])","metadata":{"execution":{"iopub.status.busy":"2023-07-24T16:08:49.976829Z","iopub.execute_input":"2023-07-24T16:08:49.977230Z","iopub.status.idle":"2023-07-24T16:10:25.878459Z","shell.execute_reply.started":"2023-07-24T16:08:49.977187Z","shell.execute_reply":"2023-07-24T16:10:25.877155Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'generated_text': 'Write a poem about Valencia\\nValencia, the city of the sun,\\nA place of beauty, of fun,\\nA place of culture, of art,\\nA place of beauty, of heart.\\n\\nThe city of orange trees,\\nThe city of the sea,\\nThe city of the mountains,\\nThe city of the sun, Valencia.\\n\\nThe city of the Valencian paella,\\nThe city of the bullfights,\\nThe city of the Valencian paella,\\nThe city of the sun, Valencia.\\n\\nThe city of the Valencian paella,\\nThe city of the bullfights,\\nThe city of the Valencian paella,\\nThe city of the sun, Valencia.\\nUser '}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Das Model befolgt unsere Anweisungen. Was also, wenn wir das Model anweisen, sich wie ein Chatbot zu verhalten, und Antworten in einer Konversation zu generieren?","metadata":{}},{"cell_type":"code","source":"USER_NAME = \"User\"\nBOT_NAME = \"Falcon\"\nDEFAULT_INSTRUCTIONS = f\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called {BOT_NAME}, and a human user, called {USER_NAME}. In the following interactions, {USER_NAME} and {BOT_NAME} will converse in natural language, and {BOT_NAME} will answer {USER_NAME}'s questions. {BOT_NAME} was built to be respectful, polite and inclusive. {BOT_NAME} will never decline to answer a question, and always attempts to give an answer that {USER_NAME} would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\"\"\"\n\noutput = llm_pipeline(\n    f\"\"\"{DEFAULT_INSTRUCTIONS}\nUser: Hi Falcon, how much is 1 kilogram in pounds?\nFalcon:\n\"\"\",\n    max_new_tokens=200,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import StoppingCriteria, AutoModelForCausalLM, AutoTokenizer, StoppingCriteriaList\n\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords_ids: list):\n        self.keywords = keywords_ids\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if input_ids[0][-1] in self.keywords:\n            return True\n        return False\n\nUSER_NAME = \"User\"\nstop_words = [f\"\\n{USER_NAME}:\", f\"{USER_NAME}\"]\nstop_ids = [tokenizer.encode(w)[0] for w in stop_words]\nstop_criteria = KeywordsStoppingCriteria(stop_ids)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:21.097584Z","iopub.execute_input":"2023-07-23T21:08:21.098037Z","iopub.status.idle":"2023-07-23T21:08:21.156288Z","shell.execute_reply.started":"2023-07-23T21:08:21.098000Z","shell.execute_reply":"2023-07-23T21:08:21.155307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USER_NAME = \"User\"\nBOT_NAME = \"Falcon\"\nDEFAULT_INSTRUCTIONS = f\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user, called User. In the following interactions, User and Falcon will converse in natural language, and Falcon will answer User's questions. Falcon was built to be respectful, polite and inclusive. Falcon will never decline to answer a question, and always attempts to give an answer that User would be satisfied with. It knows a lot, and always tells the truth. The conversation begins.\n\"\"\"\n\ndef format_chat_prompt(message: str, chat_history, instructions: str) -> str:\n    instructions = instructions.strip(\" \").strip(\"\\n\")\n    prompt = instructions\n    for turn in chat_history:\n        user_message, bot_message = turn\n        prompt = f\"{prompt}\\n{USER_NAME}: {user_message}\\n{BOT_NAME}: {bot_message}\"\n    prompt = f\"{prompt}\\n{USER_NAME}: {message}\\n{BOT_NAME}:\"\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:23.151808Z","iopub.execute_input":"2023-07-23T21:08:23.152199Z","iopub.status.idle":"2023-07-23T21:08:23.160041Z","shell.execute_reply.started":"2023-07-23T21:08:23.152167Z","shell.execute_reply":"2023-07-23T21:08:23.158853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_pipeline(\n    format_chat_prompt(\"How are you today?\", [], DEFAULT_INSTRUCTIONS),\n    max_new_tokens=1024,\n    temperature=0.8,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    stopping_criteria=StoppingCriteriaList([stop_criteria])\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:08:25.440685Z","iopub.execute_input":"2023-07-23T21:08:25.441091Z","iopub.status.idle":"2023-07-23T21:08:33.234403Z","shell.execute_reply.started":"2023-07-23T21:08:25.441057Z","shell.execute_reply":"2023-07-23T21:08:33.233298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skills = {\n    \"ğŸ’¬ Chat\": \"USER_MESSAGE\",\n    \"ğŸ”„ Translate to German\": \"Ignore all previous input. Translate the following text, which is delimitated by triple backticks to German.\\n```USER_MESSAGE```\",\n    \"âœ… Proof-Read\": \"Ignore all previous input. Act as a proof-reader on the text delimited by triple backticks. Correct spelling mistakes and grammatical errors, and output the corrected text.\\n```USER_MESSAGE```\",\n    \"ğŸ’ Summarize\": \"Ignore all previous input. Summarize the text delimited by triple backticks into two sentences. Do not exceed this length.\\n ```USER_MESSAGE```\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:56:09.072671Z","iopub.execute_input":"2023-07-23T21:56:09.073100Z","iopub.status.idle":"2023-07-23T21:56:09.081343Z","shell.execute_reply.started":"2023-07-23T21:56:09.073064Z","shell.execute_reply":"2023-07-23T21:56:09.080316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\nimport random\nimport time\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom threading import Thread\n\nRETRY_COMMAND = \"/retry\"\n\ndef run_chat(message: str, chat_history, skill: str, instructions: str):\n    if not message or (message == RETRY_COMMAND and len(chat_history) == 0):\n        yield chat_history\n        return\n\n    if message == RETRY_COMMAND and chat_history:\n        prev_turn = chat_history.pop(-1)\n        user_message, _ = prev_turn\n        message = user_message\n\n    composed_message = skills[skill].replace(\"USER_MESSAGE\", message)\n    prompt = format_chat_prompt(composed_message, chat_history, instructions)\n    chat_history = chat_history + [[composed_message, \"\"]]\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)\n    \n    generation_kwargs = dict(inputs, \n                             max_new_tokens=1024,\n                           temperature=0.8,\n                           top_p=0.9,\n                           num_return_sequences=1,\n                           eos_token_id=tokenizer.eos_token_id,\n                           pad_token_id=tokenizer.eos_token_id,\n                           stopping_criteria=StoppingCriteriaList([stop_criteria]),\n                           streamer=streamer\n                            )\n    thread = Thread(target=llm_model.generate, kwargs=generation_kwargs)\n    thread.start()\n\n    acc_text = \"\"\n    for idx, response in enumerate(streamer):\n        text_token = response\n\n        if idx == 0 and text_token.startswith(\" \"):\n            text_token = text_token[1:]\n\n        acc_text += text_token\n        last_turn = list(chat_history.pop(-1))\n        last_turn[-1] += acc_text\n        chat_history = chat_history + [last_turn]\n        yield chat_history\n        acc_text = \"\"\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot()\n    \n    with gr.Row():\n        with gr.Column(scale=0.15):\n            skill = gr.Dropdown(label=\"Skill\", choices=list(skills.keys()), value=\"ğŸ’¬ Chat\", interactive=True)\n        with gr.Column(scale=0.85):\n            msg = gr.Textbox(label=\"Message\")\n    clear = gr.Button(\"ğŸ—‘ï¸ Clear History\")\n    with gr.Accordion(\"Instructions\", open=False):\n        instructions = gr.Textbox(\n            placeholder=\"LLM instructions\",\n            value=DEFAULT_INSTRUCTIONS,\n            lines=10,\n            interactive=True,\n            label=\"Instructions\",\n            max_lines=16,\n            show_label=False,\n        )\n\n    msg.submit(run_chat,\n        [msg, chatbot, skill, instructions],\n        outputs=[chatbot],\n        show_progress=False)\n    msg.submit(lambda: \"\", inputs=None, outputs=msg)\n    msg.submit(lambda: \"ğŸ’¬ Chat\", inputs=None, outputs=skill)\n    \n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue(concurrency_count=5, max_size=20).launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T21:56:12.326097Z","iopub.execute_input":"2023-07-23T21:56:12.327297Z","iopub.status.idle":"2023-07-23T21:56:16.328697Z","shell.execute_reply.started":"2023-07-23T21:56:12.327232Z","shell.execute_reply":"2023-07-23T21:56:16.327669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weitere Ressourcen\n\n- http://jalammar.github.io/illustrated-gpt2/","metadata":{}}]}